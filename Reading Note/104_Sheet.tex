\documentclass[12pt, lettersize]{book}

% format setting
\usepackage[margin=1in]{geometry}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{physics}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{chngcntr}
\usepackage{xcolor}
\usepackage{tcolorbox}


% Theorem declaration
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{nte}[thm]{Notation}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}{Corollary}[thm]

\theoremstyle{definition}
\newtheorem{dfn}[thm]{Definition}
\newtheorem*{eg}{Example}

\theoremstyle{remark}
\newtheorem*{rem}{Remark}

\tcolorboxenvironment{thm}{
	colframe=cyan, colback=cyan!5, before skip=10pt,after skip=10pt}

\tcolorboxenvironment{nte}{
	colframe=orange, colback=orange!5, before skip=10pt,after skip=10pt}

\tcolorboxenvironment{dfn}{
	colframe=orange, colback=orange!5, before skip=10pt,after skip=10pt}

\tcolorboxenvironment{lem}{
	colframe=blue, colback=blue!5, before skip=10pt,after skip=10pt}

\tcolorboxenvironment{cor}{
	colframe=blue, colback=blue!5, before skip=10pt,after skip=10pt}

\tcolorboxenvironment{eg}{
	colframe=red, colback=red!5, before skip=10pt,after skip=10pt}

\renewcommand\qedsymbol{\hfill $\blacksquare$}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\dom}{\text{dom}\,}
\newcommand{\com}{\mathsf{C}}
\newcommand{\lline}{\noindent\rule{\textwidth}{1pt}}
\DeclareMathOperator{\mesh}{mesh}

\let\oldemptyset\emptyset
\let\emptyset\varnothing

\counterwithout{equation}{chapter}

\title{MATH 104 Cheat Sheet}
\author{Wenhao Pan}
\date{\today}

\begin{document}
	\maketitle
	
	This document is a collection of all mentioned definitions, theorems, and corollaries from \emph{Elementary Analysis} by Kenneth A. Ross or Theodore Zhu's lectures of MATH 104 Summer 2021.
	
	\tableofcontents
	
	\chapter{Introduction}
	\newpage
	\section{The Set $\N$ of Natural Numbers}
	We denote the set $\{1,2,3,\dots\}$ of all \emph{positive integers} by $\N$. Each positive integer $n$ has a successor, namely $n+1$. The following is 5 properties of $\N$:
	\begin{itemize}
		\item[\textbf{N1.}] $1$ belongs to $\N$.
		\item[\textbf{N2.}] If $n\in\N$, then its successor $n+1\in\N$.
		\item[\textbf{N3.}] $1$ is not the successor of any element in $\N$.
		\item[\textbf{N4.}] If $n$ and $m$ in $\N$ have the same successor, then $n=m$.
		\item[\textbf{N5.}] A subset of $\N$ which contains $1$, and which contains $n+1$ whenever it contains $n$, must equal $\N$.
	\end{itemize}
	Axiom \textbf{N5} is the basis of mathematical induction, which asserts all the statements $P_1,P_2,P_3,\dots$ are true provided
	\begin{itemize}
		\item[(\textbf{$I_1$})] $P_1$ is true,
		\item[(\textbf{$I_2$})] $P_{n+1}$ is true whenever $P_n$ is true.
	\end{itemize}
	\newpage
	\section{The Set $\Q$ of Rational Numbers}
	\begin{dfn}
		A number is called an \emph{algebraic number} if it satisfies a polynomial equation
		\begin{displaymath}
			c_nx^n+c_{n-1}x^{n-1}+\cdots+c_1x+c_0=0
		\end{displaymath}
		where the coefficients $c_0,c_1,\dots,c_n$ are integers, $c_n\neq0$ and $n\geq1$.
	\end{dfn}
	Rational numbers are always algebraic numbers. If $r=\frac{m}{n}$ is a rational number [$m,n\in\Z$ and $n\neq0$], then it satisfies the equation $nx-m=0$.
	
	\setcounter{equation}{0}
	\begin{thm}[Rational Zeros Theorem]\label{thm:2.2}
		Suppose $c_0,c_1,\dots,c_n$ are integers and $r$ is a rational number satisfying the polynomial equation
		\begin{equation}
			c_nx^n+c_{n-1}x^{n-1}+\cdots+c_1x+c_0=0
		\end{equation}
		where $n\geq1$, $c_n\neq0$ and $c_0\neq0$. Let $r=\frac{c}{d}$ where $c,d$ are integers having no common factors and $d\neq 0$. Then $c\,|\, c_0$ and $d\,|\,c_n$.
	\end{thm}
	In other words, the only rational candidates for solutions of (1) have the form $\frac{c}{d}$ where $c$ divides $c_0$ and $d$ divides $c_n$.
	\begin{proof}
		We are given
		\begin{displaymath}
			c_n\left(\frac{c}{d}\right)^n+c_{n-1}\left(\frac{c}{d}\right)^{n-1}+\cdots+c_1\left(\frac{c}{d}\right)+c_0=0
		\end{displaymath}
		Multiply both sides by $d^n$ and obtain
		\begin{displaymath}
			c_nc^n+c_{n-1}c^{n-1}d+c_{n-2}c^{n-2}d^2+\cdots+c_2c^2d^{n-2}+c_1cd^{n-1}+c_0d^n=0
		\end{displaymath}
		Solve for $c_0d^n$ and obtain
		\begin{displaymath}
			c_0d^n=-c[c_nc^{n-1}+c_{n-1}c^{n-2}d+\cdots+c_2cd^{n-2}+c_1d^{n-1}]
		\end{displaymath}
		Since $c$ and $d^n$ have no common factors, $c$ divides $c_0$. Do the same thing and solve for $c_nc^n$ and we will see $d$ divides $c_n$.
	\end{proof}
	
	\begin{cor}\label{cor:2.3}
		Consider the polynomial equation
		\begin{displaymath}
			x^n+c_{n-1}x^{n-1}+\cdots+c_1x+c_0=0
		\end{displaymath}
		where the coefficients $c_0,c_1,\dots,c_{n-1}$ are integers and $c_0\neq0$. Any rational solution of this equation
		must be an integer that divides $c_0$.
	\end{cor}
	\begin{proof}
		By the Rational Zeros Theorem \ref{thm:2.2}, the denominator of $r$ must divide the coefficient of $x^n$, which is $1$. Thus $r$ is an integer dividing $c_0$.
	\end{proof}
	
	\newpage
	\section{The Set $\R$ of Real Numbers}
	The set $\Q$ of Rational numbers also have the following properties for addition and multiplication:
	\begin{itemize}
		\item[\textbf{A1.}] $a+(b+c)=(a+b)+c$ for all $a,b,c$.
		\item[\textbf{A2.}] $a+b=b+a$ for all $a,b$.
		\item[\textbf{A3.}] $a+0=a$ for all $a$.
		\item[\textbf{A4.}] For each $a$, there is an element $-a$ such that $a+(-a)=0$.
		\item[\textbf{M1.}] $a(bc)=(ab)c$ for all $a,b,c$.
		\item[\textbf{M2.}] $ab=ba$ for all $a,b$.
		\item[\textbf{M3.}] $a\cdot1=a$ for all $a$.
		\item[\textbf{M4.}] For each $a\neq0$, there is an element $a^{-1}$ such that $aa^{-1}=1$.
		\item[\textbf{DL}] $a(b+c)=ab+ac$ for all $a,b,c$.  
	\end{itemize}
	
	The set $\Q$ also has an order structure $\leq$ satisfying
	\begin{itemize}
		\item[\textbf{O1.}] Given $a$ and $b$, either $a\leq b$ or $b\leq a$.
		\item[\textbf{O2.}] If $a\leq b$ and $b\leq a$, then $a=b$.
		\item[\textbf{O3.}] If $a\leq b$ and $b\leq c$, then $a\leq c$.
		\item[\textbf{O4.}] If $a\leq b$, then $a+c\leq b+c$.
		\item[\textbf{O5.}] If $a\leq b$ and $0\leq c$, then $ac\leq bc$.
	\end{itemize}
	
	\begin{thm}\label{thm:3.1}
		The following are consequences of the field properties:
		\begin{enumerate}[(i)]
			\item \textcolor{red}{$a+c=b+c\implies a=b$};
			\item $a\cdot0=0$ for all $a$;
			\item $(-a)b=-ab$ for all $a,b$;
			\item $(-a)(-b)=ab$ for all $a,b$;
			\item $(ac=bc)\land(c\neq0) \implies a=b$;
			\item $ab=0\implies(a=0)\lor(b=0)$ for $a,b,c\in\R$.
		\end{enumerate}
		for $a,c,c\in\R$.
	\end{thm}
	
	\begin{thm}\label{thm:3.2}
		The following are consequences of the properties of an ordered field:
		\begin{enumerate}[(i)]
			\item $a\leq b\implies-b\leq-a$;
			\item $(a\leq b)\land(c\leq0)\implies bc\leq ac$;
			\item $(0\leq a)\land(0\leq b)\implies 0\leq ab$;
			\item $0\leq a^2$ for all $a$;
			\item $0<1$;
			\item $0<a\implies0< a^{-1}$;
			\item $0<a<b\implies0<b^{-1}<a^{-1}$;
		\end{enumerate}
		for $a,c,c\in\R$.
	\end{thm}
	Note that $a<b$ can be represented as $(a\leq b)\land(a<b)$.
	
	\begin{dfn}\label{def:3.3}
		We define
		\begin{displaymath}
			\text{$|a|=a$ if $a\geq0$  and   $|a|=-a$ if $a\leq0$}
		\end{displaymath}
	\end{dfn}
	
	An useful fact: $|a|\leq b\iff -b\leq a\leq b$.
	
	\begin{dfn}\label{def:3.4}
		For numbers $a$ and $b$ we define dist$(a,b)=|a-b|$; dist$(a,b)$ represents the \emph{distance between $a$ and $b$}.
	\end{dfn}
	
	\begin{thm}\label{thm:3.5}
		\begin{enumerate}[(i)]
			\item[]
			\item $|a|\geq0$ for all $a\in\R$.
			\item $|ab|=|a|\cdot|b|$ for all $a,b\in\R$.
			\item $|a+b|\leq|a|+|b|$ for all $a,b\in\R$. 
		\end{enumerate}
	\end{thm}
	
	\begin{cor}\label{cor:3.6}
		dist$(a,c)\leq$ dist$(a,b)+$ dist$(b,c)$ for all $a,b,c\in\R$. This is equivalent to $|a-c|\leq|b-c|+|b-c|$.
	\end{cor}
	
	\begin{thm}[Triangle Inequality]\label{thm:3.7}
		$|a+b|\leq|a|+|b|$ for all $a,b$.
	\end{thm}
	\begin{cor}[Reverse Triangular Inequality]\label{cor: reverse triangular}
		\textcolor{red}{$\big||a|-|b|\big|\leq|a-b|$ for all $a,b\in\R$.}	
	\end{cor}
	
	\begin{tcolorbox}
		\textcolor{red}{Here is one of the most important techniques in real analysis}.
		\begin{enumerate}[(a)]
			\item If $a\leq b+\epsilon$ for any $\epsilon>0$, then $a\leq b$.
			\item If $a\geq b-\epsilon$ for any $\epsilon>0$, then $a\geq b$.
			\item If $|a-b|<\epsilon$ for any $\epsilon>0$, then $|a-b|=0$.
		\end{enumerate}
	\end{tcolorbox}
	
	\newpage
	\section{The Completeness Axiom}
	The completeness axiom for $\R$ ensure us $\R$ has no "gaps".
	\begin{dfn}\label{def:4.1}
		Let $S$ be a nonempty subset of $\R$.
		\begin{enumerate}[(a)]
			\item If $S$ contains a largest element $s_0$ [that is, $s_0\in S$ and $\forall s\in S,\ s\leq s_0$], then we call $s_0$ the \emph{maximum} of $S$ and write $s_0=\max S$.
			\item If $S$ contains a smallest element $s_0$ [that is, $s_0\in S$ and $\forall s\in S,\ s\geq s_0$], then we call $s_0$ the \emph{minimum} of $S$ and write $s_0=\min S$. 
		\end{enumerate}
	\end{dfn}
	Open intervals like $(a,b)=\{x\in\R: a<x\leq b\}$ have no minimum or maximum since the endpoints $a$ and $b$ is not in the interval.
	
	\begin{dfn}\label{def:4.2}
		Let $S$ be a nonempty subset of $\R$.
		\begin{enumerate}[(a)]
			\item If a real number $M$ satisfies $s\leq M$ for all $s\in S$, then $M$ is called an \emph{upper bound} of $S$ and the set $S$ is said to be \emph{bounded above}.
			\item If a real number $m$ satisfies $m\leq s$ for all $s\in S$, then $m$ is called an \emph{lower bound} of $S$ and the set $S$ is said to be \emph{bounded below}.
			\item The set $S$ is said to be \emph{bounded} if it is bounded above and bounded below. Thus $S$ is bounded if there exist real numbers $m$ and $M$ such that $S\subseteq[m,M]$.
		\end{enumerate}
	\end{dfn}
	The maximum of a set is always an upper bound for the set. Likewise, the minimum of a set is always a lower bound for the set.
	
	\begin{dfn}
		Least Upper Bound Property (LUBP)\newline
		An ordered set $S$ has the LUBP if every nonempty subset $\mathcal{A}\subset S$ that has an upper bound has a least upper bound in $S$.
	\end{dfn}
	Note that the set $\Q$ of rational number does not satisfy the LUBP but $\R$ does. e.g. $\mathcal(A)=\{q\in\Q: q^2<2\}$.
	
	\begin{dfn}\label{def:4.3}
		Let $S$ be a nonempty subset of $\R$.
		\begin{enumerate}[(a)]
			\item If $S$ is bounded above and $S$ has a least upper bound, then we will call it the \emph{supremum} of $S$ and denote it by $\sup S$.
			\item If $S$ is bounded below and $S$ has a greatest lower bound, then we will call it the \emph{infimum} of $S$ and denote it by $\inf S$.
		\end{enumerate}
	\end{dfn}
	If $S$ is bounded above, then $M=\sup S$ if and only if (i) $s\leq M$ for all $s\in S$, and (ii) whenever $M_1<M$, there exists $s_1\in S$ such that $s_1>M_1$. Or for each $\epsilon>0$, there exists $s\in S$ such that $s>\sup S-\epsilon$.
	
	Note that for a positive set $S=\{s: s>0\}$, its infimum is not always positive. Example: $\{\frac{1}{n}: n\in\N\}$. Each element is positive but the infimum is $0$.
	
	Here are some basic facts:
	\begin{itemize}
		\item If a set $S$ has finitely many elements, then $\max S$ exists.
		\item If $\max S$ exists, then $\sup S=\max S$.
		\item For any set $S\neq\emptyset$, $\inf S\leq \sup S$
	\end{itemize}
	
	\begin{thm}[Completeness Axiom]\label{thm:4.4}
		Every nonempty subset $S$ of $\R$ that is bounded above has a least upper bound. In other words, $\sup S$ exists and is a real number.
	\end{thm}
	Note that the completeness axiom does not hold for $\Q$. 
	\begin{cor}
		Every nonempty subset $S$ of $\R$ that is bounded below has a greatest lower bound. In other words, $\inf S$ exists and is a real number.
	\end{cor}
	
	\begin{thm}[Archimedean Property]\label{thm:4.6}
		If $a>0$ and $b>0$, then for some positive integer $n$, we have $na>b$.
	\end{thm}
	\begin{cor}
		(Set $a=1$). For any $b>0$, there exists $n\in\N$ such that $n>b$
	\end{cor}
	\begin{cor}
		(Set $b=1$). For any $a>0$, there exists $n\in\N$ such that $na>1\implies \frac{1}{n}<a$.
	\end{cor}
	
	\begin{lem}\label{lem:4.7}
		If $x,y\in\R$ such that $y-x>1$, then there exists $m\in\Z$ such that $x<m<y$.
	\end{lem}
	
	
	\begin{thm}[Denseness of $\Q$]\label{thm:4.7}
		If $a,b\in\R$ and $a<b$, then there is a rational $r\in\Q$ such that $a<r<b$.
	\end{thm}
	
	\newpage
	\section{The Symbols $+\infty$ and $-\infty$}
	The symbols $+\infty$ and $-\infty$ are extremely useful even though they are \textbf{not} real numbers. So for each real number a, $-\infty<a<\infty$. If a set $S$ is not bounded above, we define $\sup S=+\infty$. Likewise, if $S$ is not bounded below, then we define $\inf S=-\infty$.
	
	We can extend real numbers to $\R\cup\{-\infty,\infty\}$. Notice that this is not a \textbf{field}, so it does not satisfy all field properties.
	
	For emphasis, we recapitulate:
	
	Let $S$ be any nonempty subset of $\R$. The \emph{symbols} $\sup S$ and $\inf S$ always make sense. If $S$ is not bounded above, then $\sup S$ is a \emph{real} number; otherwise $\sup S=+\infty$. If $S$ is bounded below, then $\inf S$ is a \emph{real} number; otherwise $\inf S=-\infty$. Moreover, we have $\inf S\leq\sup S$.
	
	\chapter{Sequences}
	\newpage
	\section{Limits of Sequences}		
	\begin{dfn}\label{def:limit}
		A sequence $(s_n)$ of real numbers is said to \textbf{converge} to the real number \emph{$s$} provided that
		\begin{displaymath}
			\forall \epsilon > 0,\ \exists N,\ n > N \Rightarrow |s_n-s| < \epsilon.
		\end{displaymath}
		If $(s_n)$ converges to $s$, we write $\lim_{n\rightarrow \infty}s_n=s$ or $s_n\rightarrow s$. $s$ is the \emph{limit} of the sequence $(s_n)$.
		A sequence that does not converge (i.e. it has no \emph{limit}) is said to \emph{diverge}.\\
		Notice that in the definition, instead of simple $\epsilon$, we can also use some other complicated forms with some extra constants like $M\epsilon,\ \frac{\epsilon}{c},\ a^2\epsilon$ and so on.
	\end{dfn}
	
	Intuitively, the definition means that no matter how small you pick $\epsilon>0$, \textbf{eventually} the sequence will stay within $\epsilon$ of $s$ at some point (the threshold $N$) and forever after.
	
	\begin{thm}
		The limit of a sequence $(s_n)$ is unique. i.e. $(\lim s_n=s) \land (lim s_n=t) \Rightarrow s=t$.
	\end{thm}
	
	\begin{thm}
		\textcolor{red}{
			\begin{itemize}
				\item If $s_n\geq a$ for all but finitely many $n$, then $\lim s_n\geq a$.
				\item If $s_n\leq b$ for all but finitely many $n$, then $\lim s_n\leq b$.
		\end{itemize}}
	\end{thm}
	\begin{rem}
		Notice that $s_n$ need to converge so that the theorem can work.
	\end{rem}
	
	\begin{thm}[Squeeze Lemma]\label{lem: squeeze}
		\textcolor{red}{If $a_n\leq s_n\leq b_n$ for all $n$ and $\lim a_n=\lim b_n=s$, then $\lim s_n=s$.}
	\end{thm}
	\begin{rem}
		Notice that $a_n$ and $b_n$ need to converge so that the theorem can work.
	\end{rem}
	
	\newpage
	
	\section{A Discussion about Proofs}
	This section gives several examples of proofs with some discussion using the definition of the limit of a sequence.
	\begin{eg}
		Prove $\lim \frac{1}{n^2}=0$.
	\end{eg}
	\emph{Discussion}. According to the definition of the limit, we need to consider an $\epsilon >0$ such that $|\frac{1}{n^2}-0|<\epsilon$ for $n>\text{some}N$.
	$|\frac{1}{n^2}-0|<\epsilon$ implies that $\frac{1}{\epsilon}<n^2 \text{or} \frac{1}{\sqrt{\epsilon}}<n$.
	Thus we can suppose $N=\frac{1}{\sqrt{\epsilon}}$ and check if we reverse our reasoning into proof, it still makes sense. 
	
	\begin{eg}
		Prove $\lim \frac{3n+1}{7n-4}=\frac{3}{7}$
	\end{eg}
	\emph{Discussion}. Just like the last example, we can start from the definition \ref{def:limit} to get a suitable $N$.
	\begin{proof}
		Let $\epsilon>0$ and $N=\frac{19}{49\epsilon}+\frac{4}{7}$, then
		\begin{align*}
			n>N &\Rightarrow 7n>\frac{19}{7\epsilon}+4\\
			&\Rightarrow \frac{19}{7(7n-4)}<\epsilon\\
			&\Rightarrow \frac{3n+1}{7n-4} - \frac{3}{7}<\epsilon\\
			&\Rightarrow \abs{\frac{3n+1}{7n-4} - \frac{3}{7}}<\epsilon\qquad \text{since}\ n>0
		\end{align*}
		This proofs $\lim \frac{3n+1}{7n-4}=\frac{3}{7}$ according to the definition of the limit \ref{def:limit}.
	\end{proof}
	
	\begin{eg}
		Prove $\lim\frac{4n^3+3n}{n^3-6}=4$
	\end{eg}
	\emph{Discussion}. Since $\frac{4n^3+3n}{n^3-6}-4 = \frac{3n+24}{n^3-6}$, when $n>1$, we can find an upper bound for
	$\frac{3n+24}{n^3-6}$ so that the bound $<\epsilon \Rightarrow \abs{\frac{3n+24}{n^3-6}}<\epsilon$. Finding an upper bound for a fraction is equivalent to finding a upper bound for its numerator and a lower bound for its denominator.
	We know $3n+24\leq27n$ for $n>1$. Also we note $n^3-6\geq\frac{n^3}{2} \Rightarrow n>2$. Thus we can have $\frac{3n+24}{n^3-6}<\frac{27n}{n^3/2}<\epsilon \Rightarrow n>\sqrt{\frac{54}{\epsilon}},\ \text{provided}\ n>2$.
	\begin{proof}
		Let $\epsilon>0$ and $N=\max\{2,\sqrt{\frac{54}{\epsilon}}\}$, then
		\begin{align*}
			n>N &\Rightarrow (n>\sqrt{\frac{54}{\epsilon}})\land(n>2)\\
			&\Rightarrow (\frac{27n}{n^3/2}<\epsilon)\land(\frac{n^3}{2}\leq n^3-6)\land(27n\geq3n+24)\\
			&\Rightarrow \frac{3n+24}{n^3-6} < \frac{27n}{n^3/2} < \epsilon\\
			&\Rightarrow \abs{\frac{4n^3+3n}{n^3-6}-4} < \epsilon
		\end{align*}
		This proofs $\lim\frac{4n^3+3n}{n^3-6}=4$ according to the definition of the limit \ref{def:limit}.
	\end{proof}
	
	\begin{eg}
		Show that $a_n=(-1)^n$ does not converge.
	\end{eg}
	\emph{Discussion}. Assume $\lim(-1)^n=a$, and we can see that no matter what $a$ is, either $1$ or $-1$ is at least
	$1$ from $a$, so it means $\abs{(-1)^n-a}<1$ will not hold for all large $n$.
	\begin{proof}
		Suppose $\lim(-1)^n=a$ and $\epsilon = 1$. By \ref{def:limit}, $\abs{(-1)^n-a}<1 \Rightarrow (|1-a|<1)\land(|-1-a|<1)$.
		Now by \ref{def:tri-ineq}, $2=|1-a+a-(-1)|\leq|1-a|+|a-(-1)|<1+1=2$, which is a contradiction. 
	\end{proof}
	
	\begin{eg}
		Let $(s_n)$ be a sequence of nonnegative real numbers and suppose $s=\lim s_n$. Note $s\geq0$. Prove $\lim\sqrt{s_n}=\sqrt{s}$
	\end{eg}
	\begin{proof}
		There are two cases.
		\begin{enumerate}
			\item $s>0$: Let $\epsilon>0$. $\lim s_n=s \Rightarrow (\exists N,\ n>N \Rightarrow |s_n-s|<\sqrt{s}\epsilon)$.
			$n>N$ also implies
			\begin{displaymath}
				|\sqrt{s_n}-\sqrt{s}|=\frac{(\sqrt{s_n}-\sqrt{s})(\sqrt{s_n}+\sqrt{s})}{\sqrt{s_n}+\sqrt{s}}=\frac{|s_n-s|}{\sqrt{s_n}+\sqrt{s}}\leq\frac{|s_n-s|}{\sqrt{s}}<\frac{\sqrt{s}\epsilon}{\sqrt{s}}=\epsilon
			\end{displaymath}
			\item $s=0$: EXERCISE 8.3 
		\end{enumerate}
	\end{proof}
	
	\begin{eg}
		Let $(s_n)$ be a convergent sequence of real numbers such that $s_n\neq0$ for all $n\in \mathbb{N}$ and $\lim s_n=s
		\neq0$. Prove $\inf\{|s_n|: n\in\mathbb{N}\}>0$
	\end{eg}
	\begin{proof}
		Let $\epsilon=\frac{|s|}{2}$. Since $\lim s_n=s$,
		\begin{displaymath}
			n>N \Rightarrow |s_n-s|<\frac{|s|}{2} \Rightarrow |s_n|\geq\frac{|s|}{2}
		\end{displaymath}
		The last implication is because otherwise 
		\begin{displaymath}
			|s|=|s-s_n+s_n|\leq|s-s_n|+|s_n|<\frac{|s|}{2}+\frac{|s|}{2}=|s|
		\end{displaymath}
		which is a contradiction. Now if we set $m=\min\{\frac{|s|}{2},|s_1|,|s_2|,\dots,|s_N|\}$, then clearly we have
		$m>0$ since and $|s_n|\geq m$ for all $n\in \mathbb{N}$. Thus $\inf\{|s_n|: n\in\mathbb{N}\}\geq m>0$\textbf{WHY???} 
	\end{proof}
	\newpage
	
	\section{Limit Theorems for Sequences}
	\begin{dfn}\label{def:bound}
		A sequence $(s_n)$ is said to be \emph{bounded} if $\exists M,\ \forall n,\ \text{such that}\ |s_n|\leq M$
	\end{dfn}
	
	\begin{thm}\label{def:convergence is bounded}
		Convergent sequences are bounded.
	\end{thm}
	\begin{rem}
		In other words, unbounded sequences are not convergent.
	\end{rem}
	
	\begin{thm}
		If the sequence $(s_n)$ converges to $s$ and $k\in\mathbb{R}$, then $(ks_n)$ converges to $ks$. i.e. $\lim(ks_n)=k\cdot\lim s_n$.
	\end{thm}
	
	\begin{thm}\label{def:addition}
		If $(s_n)$ and $(t_n)$ converge to $s$ and $t$, then $(s_n+t_n)$ converges to $s+t$. That is,
		\begin{displaymath}
			\lim(s_n+t_n)=\lim s_n+\lim t_n.
		\end{displaymath} 
	\end{thm}
	
	\begin{thm}\label{def:multiplication}
		If $(s_n)$ and $(t_n)$ converge to $s$ and $t$, then $(s_nt_n)$ converges to $st$. That is,
		\begin{displaymath}
			\lim(s_nt_n)=(\lim s_n)(\lim t_n)
		\end{displaymath} 
	\end{thm}
	
	\begin{lem}
		If $(s_n)\rightarrow s\neq0$ and $s_n\neq0$ and  for all $n$, then $\inf\{|s_n|: n\in\N\}>0$.
	\end{lem}
	
	\begin{lem}\label{def:reciprocal}
		If $(s_n)$ converges to $s$, $s_n\neq 0$ for all $n$, and $s\neq 0$, then $(1/s_n)$ converges to $1/s$.
	\end{lem}
	
	
	\begin{thm}
		Suppose $(s_n)$ and $(t_n)$ converge to $s$ and $t$. If $s\neq 0$ and $s_n\neq 0$ for all $n$, then $(t_n/s_n)$ converges to t/s.
	\end{thm}
	
	\begin{thm}
		\begin{enumerate}[(a)]
			\item[]
			\item $\lim_{n\rightarrow\infty}(\frac{1}{n^p})=0$ for $p>0$.
			\item $\lim_{n\rightarrow\infty}a^n=0$ if $|a|<1$.
			\item $\lim(n^{1/n})=1$.
			\item $\lim_{n\rightarrow\infty}a^{1/n}=1$ for $a>0$.
		\end{enumerate}
	\end{thm}
	\newpage
	
	\begin{dfn}
		For a $(s_n)$, we write $\lim s_n=+\infty$ provided for each $M>0$ there is a number $N$ wuch that $n>N\Rightarrow s_n>M$. Similarly, we write $\lim s_n=-\infty$ provided for each $M<0$ there is a number $N$ wuch that $n>N\Rightarrow s_n<M$.
	\end{dfn}
	This implies that if $\lim s_n>-\infty$, $\exists T,\ \forall n, s_n>T$. $\lim s_n<\infty$, $\exists T,\ \forall n, s_n<T$. 
	Be careful that we say $\lim s_n=+\infty$ as $(s_n)$ \textbf{diverges} to $\infty$, \textbf{not converge} to $\infty$.
	
	\begin{thm}
		Let $\lim s_n=+\infty$ and $\lim t_n>0$. Then $\lim s_nt_n=+\infty$.
	\end{thm}
	
	\begin{thm}
		For a $(s_n)$ of \emph{positive} real numbers, we have $\lim s_n=+\infty$ if and only if $\lim(\frac{1}{s_n})=0$.
	\end{thm}
	
	\begin{thm}
		Assume all $s_n\neq0$ and that the limit $L=\lim\left|\frac{s_{n+1}}{s_n}\right|$ exists.
		\begin{enumerate}[(a)]
			\item If $L<1$, then $\lim s_n=0$.
			\item If $L>1$, then $\lim |s_n|=+\infty$.
		\end{enumerate} 
	\end{thm}
	
	\newpage
	\section{Monotone Sequences and Cauchy Sequence}
	\begin{dfn}
		$(s_n)$ is called an \emph{increasing sequence (or nondecreasing)} if $\forall n,\ s_n\leq s_{n+1}$ and $s_n\leq s_m\ \text{whenever}\ n<m$.
		Similarly, $(s_n)$ is called an \emph{decreasing sequence (or nonincreasing)} if $\forall n,\ s_n\geq s_{n+1}$. An increasing or decreasing sequence is called \emph{monotone} or \emph{monotonic} sequence.
	\end{dfn}
	
	\begin{thm}\label{def:bounded monotone seq}
		All bounded monotone sequences converge.
	\end{thm}
	\begin{rem}
		From the proof procedure above, we can see that bounded monotone sequences \textbf{converge to its infimum or supremum} of the set of all possible values.
	\end{rem}
	
	\begin{thm}\label{def:unbounded monotone seq}
		\begin{enumerate}[(i)]
			\item[]
			\item If $(s_n)$ is an unbounded increasing sequence, then $\lim s_n=+\infty$.
			\item If $(s_n)$ is an unbounded decreasing sequence, then $\lim s_n=-\infty$.
		\end{enumerate}
	\end{thm}
	\begin{cor}
		If $(s_n)$ is monotone, then $\lim s_n$ is always meaningful. i.e. $\lim s_n=s,\ +\infty,\ \text{or}\ -\infty$.
	\end{cor}
	
	Suppose $(s_n)$ is bounded. Define $u_n=\inf\{s_m: m\geq n\}$ and $v_n=\sup{s_m: m\geq n}$. Then observe that $(u_n)$ is nondecreasing and $(v_n)$ is nonincreasing since as $n$ increases, the set has fewer elements. i.e. we have fewer choices for infimum and supremum. In general, if $A\subseteq B$, then $\inf A\geq \inf B$ and $\sup A\leq \sup B$.
	
	\begin{dfn}
		Let $(s_n)$ be a sequence in $\mathbb{R}$, define
		\begin{itemize}
			\item $\lim\sup s_n=\lim\limits_{N\rightarrow\infty}\sup\{s_n: n>N\}$
			\item $\lim\inf s_n=\lim\limits_{N\rightarrow\infty}\inf\{s_n: n>N\}$
		\end{itemize}
	\end{dfn}
	If $(s_n)$ is not bounded above. $\sup\{s_n: n>N\}=+\infty$ for all $N$ and we decree $\lim\sup s_n=+\infty$.
	Likewise, if $(s_n)$ is not bounded below. $\inf\{s_n: n>N\}=-\infty$ for all $N$ and we decree $\lim\inf s_n=-\infty$.
	
	Notice that $\lim\sup s_n$ need not equal to $\sup\{s_n: n>N\}$, but $\lim\sup s_n\leq\sup\{s_n: n>N\}$.
	\begin{rem}
		Since $v_n$ and $u_n$ are monotone, $\lim v_n=\lim\sup s_n$ and $\lim u_n=\lim\inf s_n$ always exist.
	\end{rem}
	\newpage
	
	\begin{thm}\label{def:condition for limit}
		Let $(s_n)$ be a sequence in $\mathbb{R}$.
		\begin{enumerate}[(i)]
			\item If $\lim s_n$ is defined, then $\lim\inf s_n=\lim s_n=\lim\sup s_n$.
			\item If $\lim\inf s_n=\lim\sup s_n$, then $\lim s_n$ is defined and $\lim s_n=\lim\inf s_n=\lim\sup s_n$.
		\end{enumerate}
	\end{thm}
	
	\begin{dfn}\label{def:cauchy-seq}
		A $(s_n)$ is called a \emph{Cauchy sequence} if 
		\begin{displaymath}
			\forall\epsilon>0,\ \exists N\ \text{such that}\ m,n>N\Rightarrow|s_n-s_m|<\epsilon
		\end{displaymath}
	\end{dfn}
	\begin{lem}
		Convergent sequences are Cauchy sequences.
	\end{lem}
	
	\begin{lem}
		Cauchy sequences are bounded.
	\end{lem}
	
	\begin{thm}\label{def:cauchy iff convergent}
		A sequence in $\R$ is a convergent sequence if and only if it is a Cauchy sequence.
	\end{thm}
	\newpage
	
	\section{Subsequences}
		\begin{dfn}
			Suppose $(s_n)_{n\in\mathbb{N}}$ is a sequence. A \emph{subsequence} of this sequence is $(t_k)_{k\in\mathbb{N}}$ where for each $k$ there is a positive integer $n_k$ such that
			\begin{equation*}
				n_1<n_2<\cdots<n_k<n_{k+1}<\cdots
			\end{equation*}
			and
			\begin{equation*}
				t_k=s_{n_k}.
			\end{equation*}
			Thus $(t_k)$ is just a selection of some [possibly all] of the $s_n$'s taken in order.
		\end{dfn}
		For the subset $\{n_1,n_2,\dots\}$ there is a natural function $\sigma$ given by $\sigma(k)=n_k$ for $k\in\mathbb{N}$. The function $\sigma$ "selects" an infinite subset of $\mathbb{N}$ in order. Then
		the subsequence of $s$ corresponding to $\sigma$ is simply the composite function $t=s\circ\sigma$. That is
		\begin{displaymath}
			t_k=t(k)=s\circ\sigma(k)=s(\sigma(k))=s(n_k)=s_{n_k}\quad\text{for}\quad k\in N.
		\end{displaymath}
		Notice that $\sigma$ needs to be an \emph{increasing} function.
		 
		Recall that the set $\Q$ of rational numbers is \emph{countable}: there is a bijection from $\N$ to $\Q$. Therefore we have a sequence $(q_n)=(q_1,q_2,q_3,\dots)$ such that $\{q_n: n\in\N\}=\Q$. Then we have the following proposition:
		\begin{thm}
			Let $(q_n)$ be an enumeration of $\Q$. Then for any $a\in\R$, there exists a subsequence $(q_{n_k})$ of $(q_n)$ such that $q_{n_k}\rightarrow a$.
		\end{thm}
	
		\setcounter{equation}{0}
		\begin{thm}\label{def:limit-subseq}
			Let $(s_n)$ be a sequence in $\R$.
			\begin{enumerate}[(i)]
				\item If $t$ is in $\mathbb{R}$ then there is a subsequence of $(s_n)$ converging to $t$ if and only if
				the set $\{n\in\mathbb{N}: |s_n-t|<\epsilon\}$ is \emph{infinite} for all $\epsilon>0$.
				\item If $(s_n)$ is unbounded above, it has a subsequence with limit $+\infty$.
				\item If $(s_n)$ is unbounded below, it has a subsequence with limit $-\infty$.
			\end{enumerate}
			In each case, the subsequence can be taken to be \emph{monotonic}.
		\end{thm}
		
		\begin{thm}\label{def:subsequence converges to the same limit}
			If $(s_n)$ in $\R$ converges, then every subsequence converges to the same limit. If there are two subsequences of $(s_n)$ with different limits, $(s_n)$ does not converge.
		\end{thm}
		
		\begin{thm}
			Every sequence $(s_n)$ in $\R$ has a monotonic subsequence.
		\end{thm}
		\setcounter{equation}{0}
		
		\begin{thm}[Bolzano-Weierstrass Theorem]\label{def:B-W}
			Every bounded sequence in $\R$ has a convergent subsequence.
		\end{thm}
		
		\begin{dfn}
			Let $(s_n)$ be a sequence in $\mathbb{R}$. A \emph{subsequential limit} is any real number or symbol $+\infty$ or $-\infty$ that is the limit of some subsequence of $(s_n)$.
		\end{dfn}
		
		\setcounter{equation}{0}
		\begin{thm}\label{def:subsequence with limit limsup or liminf}
			Let $(s_n)$ be any sequence. There exists a monotonic subsequence whose limit is $\lim\sup s_n$, and there exists a monotonic subsequence whose limit is $\lim\inf s_n$.
		\end{thm}
		
		\begin{thm}\label{def:subsequential limit condition}
			Let $(s_n)$ be any sequence in $\mathbb{R}$, and let $S$ denote the set of subsequential limits of $(s_n)$.
			\begin{enumerate}[(i)]
				\item S is nonempty.
				\item $\sup S=\lim\sup s_n$ and $\inf S=\lim\inf s_n$.
				\item $\lim s_n$ exists if and only if $S$ has exactly one element, namely $\lim s_n$.
				\item $\lim\sup s_n\in S$ and $\lim\inf s_n\in S$.
			\end{enumerate}
		\end{thm}
		
		\begin{thm}
			Let $S$ denote the set of subsequential limits of a sequence $(s_n)$. Suppose $(t_n)$is a sequence in $S\cap\mathbb{R}$ and that $t=\lim t_n$. Then $t$ belongs to $S$.
		\end{thm}
	
		\newpage
		
	\section{lim sup's and lim inf's}
		\setcounter{equation}{0}
		\begin{thm}\label{thm:12.1}
		If $(s_n)$ converges to a positive real number $s$ and $(t_n)$ is any sequence, then
		\begin{displaymath}
			\lim\sup s_nt_n=s\cdot\lim\sup t_n.
		\end{displaymath}
		Here we allow the conventions $s\cdot(+\infty)=+\infty$ and $s\cdot(-\infty)=-\infty$ for $s>0$.
		\end{thm}
		
		\setcounter{equation}{0}
		\begin{thm}\label{thm:12.2}
		Let $(s_n)$ be any sequence of nonzero real numbers. Then we have
		\begin{displaymath}
		\lim\inf\abs{\frac{s_{n+1}}{s_n}}\leq\lim\inf|s_n|^{1/n}\leq\lim\sup|s_n|^{1/n}\leq\lim\sup\abs{\frac{s_{n+1}}{s_n}}
		\end{displaymath}
		\end{thm}
		
		\begin{cor}\label{def:12.3}
		If $\lim\abs{\frac{s_{n+1}}{s_n}}$ exists [and equals L], then $\lim|s_n|^{1/n}$ exists [and equals L].
		\end{cor}
		
		\newpage
	\section{Some Topological Concepts in Metric Spaces}
	\begin{dfn}
		Let $X$ be a set, and suppose $d$ is a function $d: X\times X\rightarrow[0,\infty]$ defined for all pairs $(x,y)$ of elements from $X$ satisfying
		\begin{enumerate}
			\item $d(x,x)=0$ for all $x\in S$ and $d(x,y)>0$ for distinct $x,y\in X$. (Positive Definiteness)
			\item $d(x,y)=d(y,x)$ for all $x,y\in X$. (Symmetry)
			\item $d(x,z)\leq d(x,y)+d(y,z)$ for all $x,y,z\in X$. (Triangle Inequality)
		\end{enumerate}
		Such a function $d$ is called a \emph{distance function} or a \emph{metric} on $X$. A \emph{metric space} $X$ is a set $X$ together with a metric on it.
	\end{dfn}
	\begin{rem}
		The positive definiteness can be also expressed as $\forall x,y\in X\ d(x,y)\geq 0$ and $d(x,y)=0\iff x=y$. The distance function cannot be $+\infty$.
	\end{rem}
	\begin{eg}[Discrete Metric Space]
		Discrete metric space is defined as
		\begin{displaymath}
			\text{For any set $X$ with metric or distance function as}\begin{cases}
				1\quad\text{$x\neq y$}\\ 0\quad\text{$x=y$}.
			\end{cases}
		\end{displaymath}
		Notice that all sets in discrete metric space are both open and closed.
	\end{eg}
	\begin{rem}
		\begin{itemize}
			\item Discrete metric space is complete but not compact.
			\item Any set in discrete metric space is open because for each element there is an open ball only contains itself, so the open ball is just the element and hence trivially contained in the set.
			\item Any set in discrete metric space is closed because it does not have any limit point. Then the set of limit points is empty and trivially contained in the set.
		\end{itemize}
	\end{rem}
	
	\begin{dfn}[Convergence]
		A sequence $(x_n)$ in a metric space $(X,d)$ converges to $x$ in $X$ if $\lim_{n\rightarrow\infty}d(x_n,x)=0$.
	\end{dfn}
	\begin{rem}
		\begin{itemize}
			\item In other words, a sequence $(x_n)$ converges to $x$ if for any $\epsilon>0$, there exists $N\in\N$ such that $n\geq N\implies d(x_n,x)<\epsilon$.
			\item Convergent sequence is Cauchy.
		\end{itemize}
	\end{rem}
	
	\begin{dfn}[Cauchy]
		A sequence $(x_n)$ in $X$ is a \emph{Cauchy} if for any $\epsilon>0$ there exists an $N\in\N$ such that
		\begin{displaymath}
			m,n\geq\implies d(x_m,x_n)<\epsilon.
		\end{displaymath}
	\end{dfn}
	
	\begin{dfn}[Complete]
		A metric space $(X,d)$ is \emph{complete} if every Cauchy sequence in $X$ converges to a point in $X$.
	\end{dfn}
	\begin{rem}
		\begin{itemize}
			\item Every convergent sequence $(x_n)$ in $X$ is Cauchy.
			\item $\Q$ is not complete. 
		\end{itemize}
	\end{rem}
	
	\begin{dfn}[Open Ball]
		Let $(X,d)$ be a metrc space. For $x\in X$ and $r>0$, the open ball of radius $r$ centered at $x$ is the set
		\begin{displaymath}
			B_r(x)=\{y\in X: d(y,x)<r\}
		\end{displaymath}
	\end{dfn}
	
	\begin{dfn}[Interior Point]
		Let $(X,d)$ be a metric space. Let $E$ be a subset of $X$. An element $x\in E$ is \emph{interior} to $E$ if for some $r>0$ we have
		\begin{displaymath}
			B_r(x)\subseteq E
		\end{displaymath}
		We write $E^\circ$ for the set of points in $E$ that are interior to $E$.
	\end{dfn}
	\begin{rem}
		\begin{itemize}
			\item The relationship between $E$ and $X$ may affect whether a point in $E$ is interior to $E$. For example, for $E=[0,1]\subset[-1,2]=X$, $0$ is not interior to $[0,1]$. However if $E=[0,1]\subset[0,1]=X$, then $0$ is interior to $0$ since there is not point in $X$ beyond the left of $0$.
			\item $E^\circ$ is open.
			\item $E=E^\circ$ if and only if $E$ is open.
			\item If $F$ is an open set such that $F\subseteq E$, then $F\subseteq E^\circ$.
		\end{itemize}		
	\end{rem}
	
	\begin{dfn}[Open Set]
		A set $E\subseteq X$ is \emph{open} if every point $x\in E$ is an interior point of $E$. i.e., if $E=E^\circ$.	
	\end{dfn}
	\begin{rem}
		\begin{itemize}
			\item[]
			
			\item A set being open does \textbf{not} mean it is \textbf{not} closed. e.g. $[0,1)$ is neither open nor closed.
		\end{itemize}
	\end{rem}
	\begin{eg}
		\begin{itemize}
			\item[]
			\item $(a,b),(a,\infty),(-\infty,a)$ are open sets.
			\item In $\R$, $\Q$ is \emph{not} open since $B_r(q)$ may contain irrational numbers in $\R$ so $B_r(q)\nsubseteq\Q$.
			\item In any metric space $(X,d)$, $X$ and $\emptyset$ are open trivially.
		\end{itemize}
	\end{eg}
	
	\begin{thm}[Open ball is open]
		Let $(X,d)$ be a metric space. Given $x\in X$ and $r>0$, $B_r(x)$ is an open set in $X$.
	\end{thm}
	
	\begin{thm}[Union and intersection of open sets]\label{thm: Union and intersection of open sets}
		Let $(X,d)$ be a metric space.
		\begin{enumerate}[(i)]
			\item If $\{\mathcal{U}_\alpha\}_{\alpha\in \mathcal{A}}$ is any collection of open sets in $X$, then $\bigcup_{\alpha\in \mathcal{A}}\mathcal{U}_\alpha$ is open. i.e. the union of \emph{any} collection of open sets is open.
			\item If $\{\mathcal{U}_1,\dots,\mathcal{U}_n\}$ is a finite collection of open sets in $X$, then $\bigcap_{i=1}^{n}\mathcal{U}_i$ is open.
		\end{enumerate}
	\end{thm}
	\begin{rem}
		The examples for infinite collection in (ii) is $\bigcap_{n=1}^{\infty}(1-\frac{1}{n}, 1+\frac{1}{n})=\{1\}$ in $\R$. Since any open ball of $1$ in $\R$ will contain points other than $1$, $1$ is not an interior point of $\{1\}$ in $\R$.  
	\end{rem}
	
	\begin{dfn}[Complement]
		For a set $E\subseteq X$, the \emph{complement} of $E$ is the set $E^C=X\backslash E=\{x\in X: x\notin E\}$.
	\end{dfn}
	
	\begin{dfn}[Limit Point]
		For a set $E\subseteq X$, a point $x\in X$ is a \emph{limit point} of $E$ if for any $r>0$, we have that $(B_r(x)\backslash\{x\})\cap E\neq\emptyset$.\smallskip
		
		$E^\prime$ denotes the set of all limit points of $E$.
	\end{dfn}
	\begin{rem}
		\begin{itemize}
			\item[]
			\item In other words, for any radius $r>0$, no matter how small is $r$, there is some element of $E$ which sits in $B_r(x)$ other than $x$ itself. 
			\item If $E\subseteq F$, then $E^\prime\subseteq F^\prime$.
			\item $(E\cup F)^\prime=E^\prime\cup F^\prime$.
		\end{itemize}
	\end{rem}
	\begin{eg}
		\begin{itemize}
			\item[]
			\item In $\R$, the set of limit points of $(0,1)$ is $[0,1]$.
			\item In $\R$, the only limit point of $\{\frac{1}{n}: n\in\N\}$ is $0$.
			\item In $\R$, the set of limit point of $\Q$ is $\R$. 
		\end{itemize}
	\end{eg}
	
	\begin{thm}
		A point $x$ is a limit point of a set $E\subseteq X$ if and only if $x=\lim x_n$ for some sequence $x_n$ of points in $E\backslash\{x\}$. 
	\end{thm}
	
	\begin{dfn}[Isolated Point]
		For a set $E\subseteq X$, $x\in E$ is called an \emph{isolated point} if $x$ is not a limit point of $E$
	\end{dfn}
	\begin{rem}
		In other words, $x$ is an isolated point or not a limit point of $E$ if there exists a radius $r$ such that $B_r(x)$ does not contain any element of $E$ except $x$ itself.
	\end{rem}
	\begin{eg}
		\begin{itemize}
			\item[]
			\item In $\R$, every integer is an isolated point of $\mathbb{Z}$.
			\item In $\R$, the set $\Q$ has no isolated point.
			\item In $\R$, every element of $\{\frac{1}{n}: n\in\N\}$ is an isolated point. 
		\end{itemize}
	\end{eg}
	
	\begin{dfn}[Closed Set]
		A set is \emph{closed} if $E^\prime\subseteq E$.
	\end{dfn}
	\begin{dfn}[Closed Set]
		Let $(X,d)$ be a metric space. A subset $E$ of $X$ is \emph{closed} if its complement $E^\com$ is an open set.
	\end{dfn}
	\begin{rem}
		\begin{itemize}
			\item[]
			\item The above two definitions are equivalent.
			\item In other words, $E$ contains all of its limit points, or every limit point of $E$ is in $E$.
			\item In any metric space $(X,d)$, $X$ and $\emptyset$ are closed.
			\item A set being closed does \textbf{not} mean it is \textbf{not} open. e.g. $[0,1)$ is neither open nor closed.
		\end{itemize}
	\end{rem}
	\begin{eg}
		\begin{itemize}
			\item In $\R$, $[0,1]$ is closed. $[a,\infty),(-\infty,a]$ are closed.
			\item In $\R$, the set $\{\frac{1}{n}: n\in\N\}$ is not closed, but $\{\frac{1}{n}: n\in\N\}\cup\{0\}$ is closed.
			\item In any metric space, $X$ and $\emptyset$ are closed.
			\item All finite sets do not have limit point, so they are trivially closed.
		\end{itemize}
	\end{eg}
	
	\begin{thm}
		A set $E\subseteq \R$ is closed if and only if every Cauchy sequence contained in $E$ has a limit that is also an element of $E$.
	\end{thm}
	
	\begin{thm}[The set of limit points is closed]
		Let $(X,d)$ be a metric space. Let $E\subseteq X$, then $E^\prime$, (the set of limit points of $E$), is closed.
	\end{thm}
	
	\begin{thm}[Union and intersection of closed sets]
		\begin{enumerate}[(i)]
			\item[]
			\item If $\{\mathcal{E}_\alpha\}_{\alpha\in\mathcal{A}}$ is any collection of closed set, then $\bigcap_{\alpha\in\mathcal{A}}\mathcal{E}_\alpha$ is closed.
			\item If $\{\mathcal{E}_1,\dots,\mathcal{E}_n\}$ is a finite collection of closed sets in $X$, then $\bigcup_{i=1}^{n}\mathcal{E}_i$ is closed.
		\end{enumerate}
	\end{thm}
	\begin{rem}
		$\bigcup_{x\in(0,1)}\{x\}=(0,1)$ is an example to the union of infinite closed sets is open in (ii).
	\end{rem}
	\newpage
	
	The proof above uses one of DeMorgan's Laws for sets.
	\setcounter{equation}{0}
	\begin{tcolorbox}[title=\textbf{DeMorgan's Laws for sets}]
		Suppose a metric space $(X,d)$ and let $\forall\alpha\in\mathcal{A}\ U_\alpha\in X$. Then $\bigcap_{\alpha\in\mathcal{A}}\mathcal{U}_{\alpha}^{\com}=\left(\bigcup_{\alpha\in\mathcal{A}}\mathcal{U}_\alpha\right)^\com$.
	\end{tcolorbox}\medskip
	
	\begin{dfn}[Bounded Set]
		A set $E\subseteq X$ is bounded if for some $x\in X$ and $M>0$ such that $d(x,y)\leq M$ for all $y\in E$.
	\end{dfn}
	\begin{rem}
		\begin{itemize}
			\item[]
			\item In $\R^k$, $X\subseteq\R^k$ is bounded if there exists $M>0$ such that $\forall\mathbf{x}\in X\ d(\mathbf{x},\mathbf{0})=\sqrt{x_1^2+\cdots+x_k^2}\leq M$.
			\item Finite union of bounded sets is bounded.
			\item Intersection of bounded sets is bounded.
			\item A set is bounded if it is contained in some open ball.
		\end{itemize}
	\end{rem}
	
	\begin{thm}
		In $R$, any closed and bounded sets always have maximum and minimum.
	\end{thm}
	
	\begin{dfn}[Closure]
		The \emph{closure} of $E$ in $X$ is $\bar{E}=E\cup E^\prime$.
	\end{dfn}
	\begin{rem}
		\begin{itemize}
			\item[]
			\item $\bar{E}$ is the intersection of all closed sets containing $E$.
			\item $\bar{E}$ is closed.
			\item $E=\bar{E}$ if and only if $E$ is closed.
			\item If $F$ is a closed set such that $E\subseteq F$, then $\bar{E}\subseteq F$.
			\item The union of closures of finite sets is equal to the closure of unions of the sets. i.e. $\overline{A\cup B}=\overline{A}\cup\overline{B}$.
		\end{itemize}
	\end{rem}
	
	\begin{thm}
		For any $E\subseteq X$, its closure $\bar{E}=E\cup E^\prime$ is closed and is the smallest closed set containing $A$.
	\end{thm}
	
	\begin{dfn}[Dense Set]
		A set $E\subseteq X$ is \emph{dense} in $X$ if $\bar{E}=X$.
	\end{dfn}
	\begin{eg}
		\begin{itemize}
			\item[]
			\item $\Q$ is dense in $\R$.
			\item In any metric space $(X,d)$, $X$ is dense in $X$. 
		\end{itemize}
	\end{eg}
	
	\begin{dfn}[Dense Set]
		A set $E\subseteq X$ is dense in $X$ if and only if for any $x\in X$ and $r>0$.
		\begin{displaymath}
			B_r(x)\cap E\neq\emptyset.
		\end{displaymath} 
	\end{dfn}
	
	\setcounter{equation}{0}
	\begin{lem}\label{def:sequence in R^k}
		\begin{itemize}
			\item[]
			\item A sequence $(\mathbf{x}^{(n)})$ in $\mathbb{R}^k$ converges to $\mathbf{x}=(x_1,\dots,x_k)$ if and only if for each $j=1,2\dots,k$, the sequence $(x_j^{(n)})$ converges in $\mathbb{R}$.
			\item A sequence $(\mathbf{x}^{(n)})$ in $\mathbb{R}^k$ is a Cauchy sequence if and only if each sequence $(x_j^{(n)})$ is a Cauchy sequence in $\mathbb{R}$.
		\end{itemize}	
	\end{lem}
	
	\begin{thm}
		Euclidean k-space $\mathbb{R}^k$ is complete.
	\end{thm}
	
	\begin{thm}[Bolzano-Weierstrass in $\R^k$]
		Every bounded sequence in $\mathbb{R}^k$ has a convergent subsequence.
	\end{thm}
	\begin{rem}
		In any general metric space $(X,d)$, it is not true that any bounded sequence has a convergent subsequence. E.g. $(\Q,d)$ and infinite discrete metric space.
	\end{rem}
	
	\begin{thm}
		Let $E$ be a subset of a metric space $(S,d)$.
		\begin{enumerate}
			\item $E$ is closed $\iff$ $E=E^-$.
			\item $E$ is closed $\iff$ $E$ contains the limit of every convergent sequence of points in $E$.
			\item An element is in $E^-$ $\iff$ it is the limit of some sequence of points in $E$.
			\item A point is in the boundary of $E$ $\iff$ it belongs to the closure of both $E$ and its complement.
		\end{enumerate}
	\end{thm}
	
	\noindent\rule{\textwidth}{1pt}
	\subsection*{Compactness}
	
	\begin{dfn}[Open Cover]
		Let $(X,d)$ be a metric space and $E\subseteq X$. An open cover of $E$ is a collection of open sets $\{\mathcal{U}_\alpha\}_{\alpha\in\mathcal{A}}$ such that $E\subseteq\bigcup_{\alpha\in\mathcal{A}}\mathcal{U}_\alpha$. An open cover is finite if it contains finitely many sets.
	\end{dfn}
	
	\begin{dfn}[Subcover]
		A subcover of an open cover $\{\mathcal{U}_\alpha\}_{\alpha\in\mathcal{A}}$ of $E$ is an \emph{open cover} $\{\mathcal{U}_\alpha\}_{\alpha\in\mathcal{B}}$ such that $\mathcal{B}\subseteq\mathcal{A}$.
	\end{dfn}
	
	\begin{dfn}[Compact Set]
		A set $E\subseteq X$ is compact if every open cover of $E$ has a \emph{finite} subcover.
	\end{dfn}
	\begin{eg}
		\begin{itemize}
			\item[]
			\item Every finite set is compact.
			\item Infinite discrete metric space is not compact.
			\item $\R$ is not compact: $\{(-n,n)\}_{n\in\N}$ is an open cover of $\R$ but does not have a finite subcover.
			\item $(0,1)$ is not compact: $\{(0,r)\}_{r\in(0,1)} $ is an open cover of $(0,1)$ but does not have a finite subcover.
			\item Closed interval in $R$ is compact.
		\end{itemize}
	\end{eg}
	
	\begin{thm}
		Compact sets are closed in any metric space.
	\end{thm}
	\begin{rem}
		Non-closed sets are not compact in any metric space. Notice open set does not mean non-closed.
	\end{rem}
	
	\begin{thm}
		Closed subsets of compact sets are compact.
	\end{thm}
	\begin{cor}
		If $\{K_\alpha\}_{\alpha\in A}$ is a collection of compact sets, then $\bigcap_{\alpha\in\mathcal{A}}K_\alpha$ is compact.
	\end{cor}

	\begin{rem}
		Finite union of compact sets in $X$ is compact.
	\end{rem}
	
	\begin{thm}
		Every sequence in a compact set has a convergent subsequence.
	\end{thm}
	
	\begin{thm}[Compact Set]
		A set $E\subseteq X$ is compact if and only if every sequence in $E$ has a convergent subsequence converging to a point in $E$.
	\end{thm}
	\begin{rem}
		By this definition, $\Q$ and infinite discrete metric space are not compact. In $\Q$, there exists a sequence converging to an irrational number, then all of its subsequences converging to irrational number which is not in $\Q$. In infinite discrete metric space, make a sequence with all distinct terms, then all of its subsequences have distinct terms. Thus all subsequences are not Cauchy and hence not convergent.
	\end{rem}
	
	\begin{thm}[Nested Compact Sets Property]
		Let $(F_n)$ be a sequence of closed, bounded, nonempty sets in $\R^k$ such that $F_1\supseteq F_2\supseteq\cdots$, then $F=\bigcap_{n=1}^{\infty}F_n\neq\emptyset$ and $F$ is closed and bounded.
	\end{thm}
	
	\begin{thm}
		Suppose $\{E_\alpha\}_{\alpha\in\mathcal{A}}$ is a collection of compact sets such that $\bigcap_{\alpha\in\mathcal{B}}E_\alpha\neq\emptyset$ for any finite $\mathcal{B}\subseteq \mathcal{A}$. Then $\bigcap_{\alpha\in\mathcal{A}}E_\alpha\neq\emptyset$.
	\end{thm}
	
	\begin{dfn}[K-cell]
		A K-cell is a subset of $\R^k$ of the form $[a_1,b_1]\times[a_2,b_2]\times\cdots\times[a_k,b_k]$.
	\end{dfn}
	
	\begin{thm}
		Every k-cell $F$ in $\mathbb{R}^k$ is compact.
	\end{thm}
	
	\begin{thm}
		A subset $E$ of $\mathbb{R}^k$ is compact if and only if it is closed and bounded.
	\end{thm}
	\begin{rem}
		The forward direction is true in any metric space.
	\end{rem}
	
	\begin{tcolorbox}[title=\textbf{Characterization of compact sets}]
		(1) and (2) are equivalent in any metric space. Forward direction of (3) is true in any metric space. All of three are equivalent in $\R^k$.
		\begin{enumerate}
			\item Every open cover of $E$ has a finite subcover.
			\item A set $E\subseteq X$ is compact if and only if every sequence in $E$ has a convergent subsequence converging to a point in $E$.
			\item A subset $E$ of $\mathbb{R}^k$ is compact if and only if it is closed and bounded.
		\end{enumerate}
	\end{tcolorbox}
	\lline
	
	\subsection*{Cantor Set}
	\begin{dfn}[Cantor Set]
		Let $\mathcal{C}_0$ be $[0,1]$. Then define $\mathcal{C}_1$ as the union of $2^1$ interval $[0,\frac{1}{3}]\cup[\frac{2}{3},1]$. Each time delete the middle $\frac{1}{3}$ of intervals. Thus $\mathcal{C}_2$ is the union of $2^2$ intervals which is $[0,\frac{1}{9}]\cup[\frac{2}{9},\frac{1}{3}]\cup[\frac{2}{3},\frac{7}{9}]\cup[\frac{8}{9},1]$.\smallskip
		
		In short, $C_n$ is the union of $2^n$ disjoint closed intervals of which length is $(\frac{1}{3})^n$. Then define Cantor Set
		\begin{displaymath}
			\mathcal{C}=\bigcap_{i=0}^{\infty}\mathcal{C}_i.
		\end{displaymath}
	\end{dfn}
	\begin{thm}
		Here are some facts/properties about the Cantor set $\mathcal{C}$:
		\begin{itemize}
			\item $\mathcal{C}$ is compact.
			\item $\mathcal{C}$ does not contain any intervals.
			\item $\mathcal{C}$ does not have any interior points.
			\item Every point in $\mathcal{C}$ is a limit point of $\mathcal{C}$.
			\item Every point in $\mathcal{C}$ is a limit point of $\mathcal{C}^\com$.
		\end{itemize}
	\end{thm}	
	\newpage
	
	\section{Series}
	For an infinite series $\sum_{n=m}^{\infty}a_n$, we say it \emph{converge} provided the sequence $(s_n)$ of 
	partial sums
	\begin{displaymath}
		s_n=a_m+a_{m+1}+\cdots+a_n=\sum_{k=m}^{n}a_k
	\end{displaymath}
	also converges to a real number $S$. i.e.
	\begin{displaymath}
		\sum_{n=m}^{\infty}a_n=S\quad\text{means}\quad\lim s_n=S\quad\text{or}\quad\lim\limits_{n\rightarrow\infty}\left(\sum_{k=m}^{n}a_k\right)=S
	\end{displaymath}
	A series that does not converge is said to \emph{diverge}, so $\sum_{n=m}^{\infty}a_n$ \emph{diverge} to $+\infty$
	, $\sum_{n=m}^{\infty}a_n=+\infty$, provided $\lim s_n=+\infty$. Similar for diverging to $-\infty$.
	
	If the terms in $\sum a_n$ are all nonnegative, then the corresponding partial sums $(s_n)$ form an increasing sequence, so $\sum a_n$ either converges or diverges to $+\infty$ by \ref{def:bounded monotone seq} and \ref{def:unbounded monotone seq}. In particular, $\sum |a_n|$ is meaningful for any $(s_n)$ whatever. The series
	$\sum a_n$ is said to \emph{converge absolutely} or to be \emph{absolutely convergent} if $\sum |a_n|$ converges.
	
	We use $\sum a_n$ to represent $\sum_{n=m}^{\infty}a_n$
	
	\begin{eg}[Geometric Series]
		A series of the form $\sum_{n=0}^{\infty}ar^n$ for constants $a$ and $r$ is called a geometric series. For $r\neq1$,
		the partial sums $s_n$ are given by
		\begin{displaymath}
			\sum_{k=0}^{n}ar^k=a\frac{1-r^{n+1}}{1-r}.
		\end{displaymath}
		Furthermore, if $|r|<1$, then $\lim_{n\rightarrow \infty}r^{n+1}=0$ and
		\begin{displaymath}
			\sum_{n=0}^{\infty}ar^n=\frac{a}{1-r}
		\end{displaymath}
		If $a\neq0$ and $|r|\geq1$, then $(ar^n)$ does not converge to $0$, so $\sum ar^n$ diverges.
	\end{eg}
	
	\begin{eg}
		\begin{displaymath}
			\sum_{n=1}^{\infty}\frac{1}{n^p}\quad \text{converges if and only if}\quad p>1
		\end{displaymath}
		If $p\leq1$, $\sum1/n^p=+\infty$
	\end{eg}
	
	\setcounter{equation}{0}
	\begin{dfn}\label{def:cauchy criterion}
		We say a series $\sum a_n$ satisfies the \emph{Cauchy criterion} if its sequence $(s_n)$ of partial sums is a
		Cauchy sequence which is:
		\begin{equation}
			\forall\epsilon>0,\ \exists N,\ m,n>N\Rightarrow|s_n-s_m|<\epsilon
		\end{equation}
		which is equivalent to
		\begin{equation}
			\forall\epsilon>0,\ \exists N,\ n\geq m>N\Rightarrow|s_n-s_{m-1}|<\epsilon.
		\end{equation}
		Since $s_n-s_{m-1}=\sum_{k=m}^{n}a_k$, we can write (2) as
		\begin{equation}
			\forall\epsilon>0,\ \exists N,\ n\geq m>N\Rightarrow\left|\sum_{k=m}^{n}a_k\right|<\epsilon
		\end{equation}
	\end{dfn}
	
	\begin{thm}
		A series converges $\iff$ it satisfies the Cauchy criterion. 
	\end{thm}
	\begin{cor}\label{def:If a_n converges then lim(a_n)=0}
		If a series $\sum a_n$ converges, then $\lim a_n=0$
	\end{cor}
	\begin{rem}
		If $\lim a_n\neq0$, then $\sum a_n$ does not converge.
	\end{rem}
	A useful contrapositive of this corollary is "If $\lim a_n\neq0$, then $\sum a_n$ does not converge."
	
	\setcounter{equation}{0}
	\begin{thm}[Comparison Test]\label{def:Comparison Test}
		Let $\sum a_n$ be a series where $a_n\geq0$ for all $n$.
		\begin{enumerate}[(i)]
			\item If $\sum a_n$ converges and $|b_n|\leq a_n$ for all $n$, then $\sum b_n$ converges.
			\item If $\sum a_n=+\infty$ and $b_n\geq a_n$ for all $n$, then $\sum b_n=+\infty$
		\end{enumerate}
	\end{thm}

	\begin{cor}
		Absolutely convergent series are convergent.
	\end{cor}
	
	\begin{thm}[Root Test]\label{def:Root Test}
		Let	$\sum a_n$ be a series and let $\alpha=\lim\sup|a_n|^{1/n}$. The series $\sum a_n$
		\begin{enumerate}[(i)]
			\item converges absolutely if $\alpha<1$
			\item diverges if $\alpha>1$
			\item Otherwise the test does not provide any useful information.
		\end{enumerate}
	\end{thm}
	
	\begin{thm}[Ratio Test]\label{def:Ratio Test}
		A series $\sum a_n$ of nonzero terms
		\begin{enumerate}[(i)]
			\item converges absolutely if $\lim\sup\left|\frac{a_{n+1}}{a_n}\right|<1$,
			\item diverges if $\lim\inf\left|\frac{a_{n+1}}{a_n}\right|>1$.
			\item Otherwise $\lim\inf\left|\frac{a_{n+1}}{a_n}\right|\leq1\leq\lim\sup\left|\frac{a_{n+1}}{a_n}\right|$ and
			the test gives no information.
		\end{enumerate}
	\end{thm}
	If the terms $a^n$ are nonzero and if $\lim\left|\frac{a_{n+1}}{a_n}\right|=1$, then $\alpha=\lim\sup|a_n|^{1/n}=1$
	by \ref{def:12.3}, so neither the Ratio Test nor the Root Test gives information about the convergence of $\sum a_n$.
	\newpage
	
	\section{Alternating Series and Integral Tests}
	Sometimes we can try to check convergence or divergence of series by comparing the partial sums with familiar
	integrals. By drawing the function $a^n$ and the of rectangles corresponding to the series on a same picture and comparing the areas under the function and the sum of areas of these rectangles, we may get the information about the convergence of the series. For example, if all rectangles are below the function and the integral of the function is finite, then the series converge.
	\begin{thm}
		$\sum\frac{1}{n^p}$ converges $\iff$ $p>1$.
	\end{thm}
	
	\begin{thm}
		Here are the conditions under which an integral test is advisable:
		\begin{enumerate}[(a)]
			\item All comparison, root, and ratio tests do not apply.
			\item The terms $a_n$ of the series are nonnegative.
			\item There is a nice decreasing function $f$ on $[1,\infty)$ such that $f(n)=a_n$ for all $n$.
			\item The integral of $f$ is easy to calculate or estimate.
		\end{enumerate}
		If $\lim_{n\rightarrow\infty}\int_{1}^{n}f(x)dx=+\infty$, then the series diverges. If $\lim_{n\rightarrow\infty}\int_{1}^{n}f(x)dx<+\infty$, then the series will converge.
	\end{thm}
	
	\begin{thm}[Alternating Series Theorem]\label{def:alternating series}
		If $a_1\geq a_2\geq\cdots\geq a_n\geq\cdots\geq0$ and $\lim a_n=0$, then the alternating series $\sum(-1)^{n+1}a_n$
		converges. Moreover, the partial sums $s_n=\sum_{k=1}^{n}(-1)^{k+1}a_k$ satisfy $|s-s_n|\leq a_n$ for all $n$.
	\end{thm}
	
	\newpage
	
	\chapter{Continuity}
		\newpage
		\section{Continuous Functions}
		In this book/note, we will be concerned with functions $f$ such that $\dom(f)\subseteq\R$ and such that $f$ is
		a real-valued function. We consider the \emph{natural domain} as "the largest subset of $\R$ on which the function is a well defined real-valued function. 
		
		\begin{dfn}\label{def:continuous}
			The function $f$ is \emph{continuous} at $x_0$ in $\dom(f)$ if, for every sequence $(x_n)$ in $\dom(f)$ converging to $x_0$, we have $\lim_nf(x_n)=f(x_0)$. If $f$ is continuous at each point of a set $S\subseteq\dom(f)$, then $f$ is said to be \emph{continuous} on $S$. The function $f$ is said to be
			\emph{continuous} if it is continuous on $\dom(f)$.
		\end{dfn}
		
		\setcounter{equation}{0}
		\begin{thm}\label{def:delta-epsilon property}
			$f$ is continuous at $x_0$ in $\dom(f)$ if and only if
			\begin{equation}
				\forall\epsilon>0,\ \exists\delta>0\quad\text{such 	that}\quad(x\in\dom(f))\land(|x-x_0|<\delta)\implies|f(x)-f(x_0)|<\epsilon
			\end{equation}
		\end{thm}
		The condition $(x\in\dom(f))\land(|x-x_0|<\delta)\implies|f(x)-f(x_0)|<\epsilon$ in the book is a little bit
		confusing. In other words, it means
		\begin{displaymath}
			\forall x\in\dom(f),\ |x-x_0|<\delta\implies|f(x)-f(x_0)|<\epsilon
		\end{displaymath}
		
		\begin{cor}[Discontinuity]
			To use $\epsilon\text{-}\delta$ property to prove the discontinuity, we need to show that
			\begin{displaymath}
				\exists\epsilon>0\quad\text{such that}\quad\forall\delta>0,\ \exists x\in\dom(f)\quad\text{such that}\quad |x-x_0|<\delta\quad\text{but}\quad|f(x)-f(x_0)|\geq\epsilon
			\end{displaymath}
		\end{cor}
		
		\begin{thm}\label{def:17.3}
			If $f$ is continuous at $x_0$ in $\dom(f)$, then $|f|$ and $kf$, for $k\in\R$, are continuous at $x_0$.
		\end{thm}
		
		\begin{thm}\label{def:17.4}
			Let $f$ and $g$ be real-valued fucntions that are continuous at $x_0$ in $\R$. Then
			\begin{enumerate}[(i)]
				\item $f+g$ is continuous at $x_0$;
				\item $fg$ is continuous at $x_0$;
				\item $f/g$ is continuous ar $x_0$ if $g(x_0)\neq0$.
			\end{enumerate}
		\end{thm}
		
		\begin{thm}\label{def:17.5}
			If $f$ is continuous at $x_0$ and $g$ is continuous at $f(x_0)$, then the composite function $g\circ f$ is
			continuous at $x_0$.
		\end{thm}
		
		\begin{tcolorbox}[title=Conclusion]
			Suppose $f,g$ are real-valued functions. If $f$ and $g$ are continuous at $x_0$, then the following functions are also continuous at $x_0$ (as long as $x_0$ is also in the domain of them):
			\begin{itemize}
				\item $(f+g)(x_0)=f(x_0)+g(x_0)$
				\item $(fg)(x_0)=f(x_0)g(x_0)$
				\item $k\in\R\ (kf)(x_0)=k(f(x_0))$
				\item $(f/g)(x_0)=f(x_0)/g(x_0)$
				\item $(|f|)(x_0)=|f(x_0)|$
				\item $(\max\{f,g\})(x_0)=\max\{f(x_0),g(x_0)\}$
				\item $(\min\{f,g\})(x_0)=\min\{f(x_0),g(x_0)\}$
			\end{itemize}
		\end{tcolorbox}
		
		\newpage
		
		\section{Properties of Continuous Functions}
		A real-valued function $f$ is said to be \emph{bounded} if $\{f(x): x\in\dom(f)\}$ is a bounded set. i.e. if
		there exists a real number $M$ such that $|f(x)|\leq M$ for all $x\in\dom(f)$.
		
		\begin{thm}\label{def:18.1}
			Let $f$ be a continuous real-valued function on a \emph{closed} interval $[a,b]$. Then $f$ is a bounded function.
			Moreover, $f$ assume its maximum and minimum values on $[a,b]$; that is there exist $x_0,y_0$ in $[a,b]$
			such that $f(x_0)\leq f(x)\leq f(y_0)$ for all $x\in[a,b]$.
		\end{thm}
		
		\begin{thm}[Intermediate Value Theorem]\label{thm:18.2}
			If $f$ is a continuous real-valued function on a closed interval $I$, then $f$ has the intermediate value property on I: Whenever $a,b\in I$, if $a<b$ and $y$ lies between $f(a)$ and $f(b)$ [i.e. $f(a)<y<f(b)$ or $f(b)<y<f(a)$], then there exists at least one $x$ in
			$(a,b)$ such that $f(x)=y$.
		\end{thm}
		\begin{cor}[Fixed Point Theorem]
			Let $f$ be a function $f:[0,1]\rightarrow[0,1]$. If $f$ is continuous, then $f$ has a fixed point, i.e., there exists $x\in[0,1]$ such that $f(x)=x$.
		\end{cor}
		\begin{cor}
			Let $f$ and $g$ be continuous functions on $[a,b]$ such that $f(a)\geq g(a)$ and $f(b)\leq g(b)$. Prove $f(x_0)=g(x_0)$ for at least one $x_0$ in $[a,b]$.
		\end{cor}
		
		\begin{cor}\label{def:18.3}
			If $f$ is a continuous real-valued function on an interval $I$, then the set $f(I)=\{f(x): x\in I\}$ is also
			an interval or a single point.
		\end{cor}
		
		\begin{thm}\label{def:18.5}
			Let $g$ be a strictly increasing function on an interval $J$ such that $g(J)$ is an interval $I$. Then $g$ is
			continuous on $J$.
		\end{thm}
		
		\begin{thm}\label{def:18.4}
			Let $f$ be a continuous strictly increasing function on some interval $I$. Then $f(I)$ is an interval $J$ by \ref{def:18.3} and $f^{-1}$ represents a function with domain $J$. The function $f^{-1}$ is a continuous strictly increasing function on $J$.
		\end{thm}
		
		\setcounter{equation}{0}
		\begin{thm}
			Let $f$ be a \emph{one-to-one} continuous function on an interval $I$. Then $f$ is strictly increasing or strictly decreasing.
		\end{thm}
		\newpage
		
		\section{Uniform Continuity}
		Sometimes we want to know when the $\delta$ in \ref{def:delta-epsilon property} can be chosen to depend only on
		$\epsilon>0$ and $S$, so that $\delta$ does not depend on the particular point $x_0$.
		\begin{dfn}
			Let $f$ be a real-valued function defined on a set $S\subseteq\R$. Then $f$ is uniformly ocntinuous on $S$ if
			\begin{align*}
				\text{for each $\epsilon>0$ there exists $\delta>0$ such that}\\
				\text{$\forall x,y\in S,\ |x-y|<\delta\implies|f(x)-f(y)|<\epsilon.$}
			\end{align*}
			We will say $f$ is uniformly continuous if $f$ is uniformly continuous on $\dom f$.
		\end{dfn}
		
		\begin{thm}
			If a real-valued function $f$ is uniformly continuous on an open interval $(a,b)$, then $f$ is \emph{bounded} on $(a,b)$.
		\end{thm}
		
		\begin{thm}\label{def:19.2}
			If $f$ is continuous on a closed interval $[a,b]$, then $f$ is uniformly continuous on $[a,b]$.
		\end{thm}
		
		\begin{thm}\label{thm:19.4}
			If $f$ is uniformly continuous on a set $S$ and $(s_n)$ is a Cauchy sequence in $S$, then $(f(s_n))$ is a 
			Cauchy sequence.
		\end{thm}
		\begin{cor}
			Let $(s_n)$ be a Cauchy sequence in $S$ and $f$ be a function on $S$. If $(f(s_n))$ is not a Cauchy sequence, then $f$ is not \emph{uniformly continuous}.
		\end{cor}
		\begin{cor}
			If $f$ is uniformly continuous on $(a,b)$, then $f$ is a bounded function on $(a,b)$. i.e. If $f$ is an unbounded function on $(a,b)$, then $f$ is not uniformly continuous on $(a,b)$.
		\end{cor}
		
		\setcounter{equation}{0}
		\begin{thm}[Continuous Extension Theorem]\label{def:19.5}
			A real-valued function $f$ on $(a,b)$ is uniformly continuous on $(a,b)$ if and only if it can be extended to
			a continuous function $\tilde{f}$ on $[a,b]$.
		\end{thm}
		
		\begin{thm}
			Let $f$ be a continuous function on an interval $I$ [$I$ may be bounded or unbounded]. Let $I^\circ$ be the
			interval obtained by removing from $I$ any endpoints that happen to be in $I$. If $f$ is differentiable on
			$I^\circ$ and if $f^\prime$ is bounded on $I^\circ$, then $f$ is uniformly continuous on $I$.
		\end{thm}
		\newpage
		
		\section{Continuity in Metric Space}
		\begin{dfn}[Image and Preimage]
			Let $X$ and $Y$ be two sets. Let a function $f: X\rightarrow Y$. Let $E\subseteq X$ and $U\subseteq Y$. We define the \emph{image of $E$ under $f$} as
			\begin{displaymath}
				f(E)=\{f(x): x\in E\};
			\end{displaymath}
			define the \emph{preimage of $A$ under $f$} as
			\begin{displaymath}
				f^{-1}(A)=\{x\in X: f(x)\in A\}.
			\end{displaymath}
		\end{dfn}
		
		\begin{thm}
			Let $X$ and $Y$ be two sets, and let $f: X\rightarrow Y$, let $E\subseteq X$, and let $A,B\subseteq Y$. Thenthe following assertions are true:
			\begin{enumerate}[(a)]
				\item $f^{-1}(A\cap B)=f^{-1}(A)\cap f^{-1}(B)$.
				\item $f^{-1}(A\cup B)=f^{-1}(A)\cup f^{-1}(B)$.
				\item $f^{-1}(A^\com)=(f^{-1}(A))^\com$.
				\item $f^{-1}(A)\subseteq f^{-1}B$ if $A\subseteq B$.
				\item $E\subseteq f^{-1}(f(E))$.
			\end{enumerate}
		\end{thm}
		
		\begin{dfn}[Continuity]\label{def:continuity (Metrics)}
			Let $(X, d_X)$ and $(Y, d_Y)$ be two metric spaces, and let $f: X\rightarrow Y$. The following are three \emph{equivalent} definitions of continuity at a point $x_0\in X$.
			\begin{enumerate}
				\item ($\epsilon$-$\delta$ definition) For any $\epsilon>0$, there exists $\delta>0$ such that $x\in X\text{ and }d_X(x,x_0)<\delta\implies d_Y(f(x),f(x_0))<\epsilon$.
				\item (sequential definition) For any sequence $(x_n)$ in $X$ converging to $x_0$, the sequence $f(x_n)$ converges to $f(x_0)$.
				\item (topological definition) For any open set $U$ in $Y$ such that $f(x_0)\in U$, there exists an open set $V$ in $X$ such that $x_0\in V\subseteq f^{-1}(U)$.
			\end{enumerate}
		\end{dfn}
		\begin{rem}
			See worksheet 12 for the proof of equivalence.
		\end{rem}
		`	
		\begin{thm}\label{thm:preimage is open}
			$f$ is continuous (on its domain) if and only if $f^{-1}(U)$ is open in $X$ for every open set $U$ in $Y$. i.e. a function is continuous if and only if the preimage of every open set is open.
		\end{thm}
		\begin{rem}
		By considering the complement of an open set, we also have that ``$f:X\rightarrow Y$ is continuous if and only if $f^{-1}(E)$ is closed in $X$ for every closed set $E$ in $Y$".
		\end{rem}
		
		\begin{dfn}[Uniform Continuity]
			Let $(X,d_X)$ and $(Y,d_Y)$ be two metric spaces. A function $f: X\rightarrow Y$ is \emph{uniformly continuous} on $E\subseteq X$ if for any $\epsilon>0$ there exists $\delta>0$ such that
			\begin{displaymath}
				x_1,x_2\in E\text{ and }d_X(x_1,x_2)<\delta\implies d_Y(f(x_1),f(x_2))<\epsilon.
			\end{displaymath} 
		\end{dfn}
		
		\begin{thm}
			Let $(X,d_X)$ and $(Y,d_Y)$ be two metric spaces. Let a function $f:X\rightarrow Y$ be continuous. Suppose a subset $E\subseteq X$ is compact.
			\begin{enumerate}[(i)]
				\item $f(E)$ is compact. i.e., the image of a compact set under continuous function is still compact.
				\item $f$ is uniformly continuous on $E$. i.e., a continuous function on a compact set is also uniformly continuous.
			\end{enumerate}
		\end{thm}
		\begin{cor}
			Let $f$ be a continuous function $f:X\rightarrow\R$. Suppose $E$ is a compact subset of $X$. Then
			\begin{enumerate}[(i)]
				\item $f(E)$ is closed and bounded;
				\item There exists $u,v\in E$ such that $f(u)=\inf_{x\in E}f(x)=\inf f(E)$ and $f(v)=\sup_{x\in E}f(x)=\sup f(E)$. i.e. $f$ attains its \emph{maximum} and \emph{minimum} on $E$.
			\end{enumerate}
		\end{cor}
		
		\begin{thm}
			Let $S$ be a subset of $\R$. Suppose a function $f:S\rightarrow\R$. If $f$ is continuous on an interval $I\subseteq S$ like $[a,b],(a,b),(a,b]$ where $a,b\in\R\cup\{\pm\infty\}$, then $f(I)$ is a singleton or an interval.
		\end{thm}
		\begin{rem}
			Singleton is a set with exactly one element.
		\end{rem}
		
		\begin{thm}[Continuous Extension Theorem]
			Let $(X,d)$ be a metric space. Let $E\subseteq X$ and $f$ be a function $f:E\rightarrow R$
			\begin{enumerate}[(i)]
				\item If $f$ is uniformly continuous function on $E$, then $f$ can be extended to a (uniformly) continuous function on $\overline{E}=E\cup E^\prime$.
				\item If $f$ can be extended to a uniformly continuous function on $\overline{E}=E\cup E^\prime$, then $f$ is uniformly continuous function on $E$.
			\end{enumerate}
		\end{thm}
		\begin{rem}
		\begin{itemize}
			\item Be careful that if $f$ is \emph{only continuous} (not uniformly) on $\overline{E}$, then (ii) fails.
			\item We can generalize the theorem further by replace the codomain $\R$ of $f$ by any complete metric space $(Y,d_Y)$.
		\end{itemize}
		\end{rem}
		\newpage
		
		\section{Limits of Functions}
		\begin{dfn}
			Let $S\subset\R$ and $a\in\R$ or a symbol $\infty$ or $-\infty$ that is the limit of some sequence in $S$, and let $L$ be a real number or symbol $+\infty$ or $-\infty$. We write $\lim_{x\rightarrow a^S} f(x)=L$ if
			\begin{displaymath}
				\text{$f$ is a function defined on $S$,}
			\end{displaymath}
			and
			\begin{displaymath}
				\text{for every sequence $(x_n)$ in $S$ with limit $a$, we have $\lim_{n\rightarrow\infty}f(x_n)=L$}.
			\end{displaymath}
		\end{dfn}
		Recall the definition of continuity, now we can say that a function $f$ is continuous at $a$ in $\dom(f)=S \iff \lim_{x\rightarrow a^S} f(x)=f(a)$. Also notice that when limits exist, they are unique. In other words, there
		is only one $L$ equals to $\lim_{x\rightarrow a^S} f(x)$.  
		
		Now let's define the various standard limit concepts for functions.
		\begin{dfn}
			\begin{enumerate}[(a)]
				\item[]
				\item For $a\in\R$ and a function $f$ we write $\lim_{x\rightarrow a}f(x)=L$ provided $\lim_{x\rightarrow a^S}f(x)=L$ for some set $S=J\backslash\{a\}$ where $J$ is an open interval containing $a$. Such a limit 
				$\lim_{x\rightarrow a^S}$ is called the \emph{[two-sided]} limit of $f$ at $a$. Note that neither $f(a)$ needs to be defined or $\lim_{x\rightarrow a}f(x)$ needs to be equal $f(a)$, unless we want to say $f$ is continuous at $a$.
				\item For $a\in\R$ and a function $f$ we write $\lim_{x\rightarrow a^+}f(x)=L$ provided $\lim_{x\rightarrow a^S}f(x)=L$ for some open interval $S=(a,b)$. This is called the \emph{[right-hand]} limit. Again $f$ need not
				be defined at $a$.
				\item For $a\in\R$ and a function $f$ we write $\lim_{x\rightarrow a^-}f(x)=L$ provided $\lim_{x\rightarrow a^S}f(x)=L$ for some open interval $S=(c,a)$. This is called the \emph{[left-hand]} limit.
				\item For a function $f$ we write $\lim_{x\rightarrow\infty}f(x)=L$ provided $\lim_{x\rightarrow\infty^S}f(x)=L$ for some interval $S=(c,\infty)$. Likewise, For a function $f$ we write $\lim_{x\rightarrow-\infty}f(x)=L$ provided $\lim_{x\rightarrow-\infty^S}f(x)=L$ for some interval $S=(-\infty,b)$
			\end{enumerate} 
		\end{dfn}
		
		\begin{thm}\label{def:20.4}
			Let $f_1$ and $f_2$ be functions for which the limits $L_1=\lim_{x\rightarrow a^S}f_1(x)$ and $L_2=\lim_{x\rightarrow a^S}f_2(x)$ exist and are finite. Then
			\begin{enumerate}[(i)]
				\item $\lim_{x\rightarrow a^S}(f_1+f_2)(x)$ exists and equals $L_1+L_2$;
				\item $\lim_{x\rightarrow a^S}(f_1f_2)(x)$ exists and equals $L_1L_2$;
				\item $\lim_{x\rightarrow a^S}(f_1/f_2)(x)$ exists and equals $L_1/L_2$ provided $L_2\neq0$ and $f_2(x)\neq0$ for $x\in S$.
			\end{enumerate}
		\end{thm}
		
		\begin{thm}\label{def:20.5}
			Let $f$ be a function for which the limit $L=\lim_{x\rightarrow a^S}f(x)$ exists and is finite. If $g$ is a function defined on $\{f(x): x\in S\}\cup\{L\}$ that is continuous at $L$, then $\lim_{x\rightarrow a^S} g\circ f(x)$ exists and equals $g(L)$.
		\end{thm}
		Be careful that for this theorem to work, $g$ needs to be \textbf{continuous} at $L$.
		
		\setcounter{equation}{0}	
		\begin{thm}\label{def:20.6}
			Let $f$ be a function defined on a subset $S$ of $\R$, let a be a real number that is the limit of some sequence
			in $S$, and let $L$ be a real number, then $\lim_{x\rightarrow a^S}f(x)=L$ if and only if
			\begin{equation}
				\text{for each $\epsilon>0$ there exists $\delta>0$ such that $x\in S$ and $|x-a|<\delta$ imply $|f(x)-L|<\epsilon$}
			\end{equation}
		\end{thm}
		
		\setcounter{equation}{0}			
		\begin{cor}\label{def:20.7}
			Let $f$ be a function defined on $J\backslash\{a\}$ for some open interval $J$ containing $a$, and let $L$ be a
			real number. Then  $\lim_{x\rightarrow a^S}f(x)=L$ if and only if
			\begin{equation}
				\text{for each $\epsilon>0$ there exists $\delta>0$ such that $0<|x-a|<\delta\implies|f(x)-L|<\epsilon$}
			\end{equation}
		\end{cor}
		
		\setcounter{equation}{0}			
		\begin{cor}\label{def:20.8}
			Let $f$ be a function defined on some interval $(a,b)$, and let $L$ be a real number. Then $\lim_{x\rightarrow a^+}f(x)=L$ if and only if
			\begin{equation}
				\text{for each $\epsilon>0$ there exists $\delta>0$ such that $a<x<a+\delta\implies|f(x)-L|<\epsilon$}
			\end{equation} 
		\end{cor}
		
		Now let's give some general conditions for the limit of function in different situations: $\lim_{x\rightarrow s}f(x)=L \iff$
		\begin{itemize}
			\item $L$ is finite:
			\begin{itemize}
				\item $s=a$: for each $\epsilon>0$ there exists $\delta>0$ such that $0<|x-a|<\delta$ implies $|f(x)-L|<\epsilon$.
				\item $s=a^+$: for each $\epsilon>0$ there exists $\delta>0$ such that $a<x<a+\delta$ implies $|f(x)-L|<\epsilon$.
				\item $s=a^-$: for each $\epsilon>0$ there exists $\delta>0$ such that $a-\delta<x<a$ implies $|f(x)-L|<\epsilon$.
				\item $s=\infty$: for each $\epsilon>0$ there exists $\alpha<\infty$ such that $x>\alpha$ implies $|f(x)-L|<\epsilon$.
				\item $s=-\infty$: for each $\epsilon>0$ there exists $\alpha>-\infty$ such that $x<\alpha$ implies $|f(x)-L|<\epsilon$.
			\end{itemize}
			\item $L=+\infty$:
			\begin{itemize}
				\item $s=a$: for each $M>0$ there exists $\delta>0$ such that $0<|x-a|<\delta$ implies $f(x)>M$.
				\item $s=a^+$: for each $M>0$ there exists $\delta>0$ such that $a<x<a+\delta$ implies $f(x)>M$.
				\item $s=a^-$: for each $M>0$ there exists $\delta>0$ such that $a-\delta<x<a$ implies $f(x)>M$.
				\item $s=\infty$: for each $M>0$ there exists $\alpha<\infty$ such that $x>\alpha$ implies $f(x)>M$.
				\item $s=-\infty$: for each $M>0$ there exists $\alpha>-\infty$ such that $x<\alpha$ implies $f(x)>M$.
			\end{itemize}
			\item $L=-\infty$:
			\begin{itemize}
				\item $s=a$: for each $M<0$ there exists $\delta>0$ such that $0<|x-a|<\delta$ implies $f(x)<M$.
				\item $s=a^+$: for each $M<0$ there exists $\delta>0$ such that $a<x<a+\delta$ implies $f(x)<M$.
				\item $s=a^-$: for each $M<0$ there exists $\delta>0$ such that $a-\delta<x<a$ implies $f(x)<M$.
				\item $s=\infty$: for each $N<0$ there exists $\alpha<\infty$ such that $x>\alpha$ implies $f(x)<N$.
				\item $s=-\infty$: for each $N<0$ there exists $\alpha>-\infty$ such that $x<\alpha$ implies $f(x)<N$. 
			\end{itemize}
		\end{itemize}
		
		\begin{thm}\label{def:20.10}
			Let $f$ be a function defined on $J\backslash\{a\}$ for some open interval $J$ containing $a$. Then $\lim_{x\rightarrow a}f(x)$ exists $\iff$ the limits $\lim_{x\rightarrow a^+}f(x)$ and $\lim_{x\rightarrow a^-}f(x)$ both exist and are equal to each other, thereby all three limits are equal.
		\end{thm}
	
	\chapter{Sequences and Series of Functions}
	\newpage
		\section{Power Series}
		\begin{dfn}
			Given a sequence $(a_n)_{n=0}^{\infty}$ of real numbers and $x_0\in\R$, the series $\sum_{n=0}^{\infty}a_n(x-x_0)^n$ is called 
			a power series, which is a function of $x$ provided it converges for some or all $x$. Note  that any power series always converges at $x=x_0$ (with convention $0^0=1$). One of the following
			holds for a power series with coefficients $(a_n)$:
			\begin{enumerate}[(a)]
				\item The power series converge for all $x\in\R$;
				\item The power series converges only for $x=x_0$;
				\item The power series converges for all $x$ in some bounded interval centered at $x_0$; the interval may be
				open, half-open, or closed.
			\end{enumerate}
		\end{dfn}
		
		\begin{thm}\label{def:23.1}
			For the power series $\sum a_n(x-x_0)^n$, let
			\begin{displaymath}
				\beta=\lim\sup |a_n|^{1/n}
			\end{displaymath}
			and
			\begin{displaymath}
				R:=\begin{cases}
					\frac{1}{\beta}\quad\text{if $0<\beta<\infty$,}\\
					\infty\quad\text{if $\beta=0$,}\\
					0\quad\text{if $\beta=\infty$.}
				\end{cases}
			\end{displaymath} Then
			\begin{enumerate}[(i)]
				\item The power series converges for $|x-x_0|<R$;
				\item The power series diverges for $|x-x_0|>R$. 
			\end{enumerate}
			We call $R$ the \emph{radius of convergence} for the power series. Note that we need to check $|x-x_0|=R$ cases individually.
		\end{thm}
		
		\begin{cor}
			If $\lim\abs{\frac{a_n}{a_n+1}}$ exists, then it is equal to the radius of convergence of the power series.
		\end{cor}
		
		\begin{dfn}[Interval of Convergence]
			The interval of convergence of the power series $\sum a_n(x-x_0)^n$ is the set $\{x\in\R:$ the series of real numbers $\sum a_n(x-x_0)^n$ converges$\}$.
		\end{dfn}
		\newpage
		\section{Uniform Convergence}
		\begin{dfn}[Pointwise Convergence]
			Let $(X,d)$ be a metric space. Let $(f_n)$ be a sequence of real-valued functions defined on a set $E\subseteq X$. The sequence $(f_n)$ \emph{converges pointwise} on $E$ to a function $f$ defined on $E$ if
			\begin{displaymath}
				\lim\limits_{n\rightarrow\infty}f_n(x)=f(x)\quad\text{for each}\quad x\in S.
			\end{displaymath}
			We often write $\lim f_n=f$ \emph{pointwise [on E]} or $f_n\rightarrow f$ \emph{pointwise [on E]}
		\end{dfn}
		Observe that saying $f_n\rightarrow f$ \emph{pointwise [on E]} is equivalent to the following:
		\begin{displaymath}
			\text{for each $\epsilon>0$ and $x$ in $E$ there exists $N$ such that $|f_n(x)-f(x)|<\epsilon$ for $n>N$.}
		\end{displaymath}
		
		\begin{dfn}[Uniform Convergence (i)]
			Let $(X,d)$ be a metric space. Let $(f_n)$ be a sequence of real-valued functions defined on a set $E\subseteq X$. The sequence $(f_n)$ \emph{converges uniformly} on $E$ to a function $f$ defined on $E$ if
			\begin{align*}
				\text{for each $\epsilon>0$ there exists a number $N$ such that}\\
				\text{$|f_n(x)-f(x)|<\epsilon$ for all $x\in E$ and all $n\geq N$.}
			\end{align*}
			We write $\lim f_n=f$ \emph{uniformly [on E]} or $f_n\rightarrow f$ \emph{uniformly [on E]}
		\end{dfn}
		\begin{rem}
			\begin{itemize}
				\item Comparing to pointwise convergence, here for each $\epsilon>0$, $N$ works for all the $x\in E$.
				\item Note that if $f_n\rightarrow f$ uniformly on $E$ and if $\epsilon>0$, then there exists $N$ such that $f(x)-\epsilon<f_n(x)<f(x)+\epsilon$ for \emph{all} $x\in E$ and $n\geq N$. i.e. for $n\geq N$ the graph of $f_n$ lies in the strip between the graphs of $f-\epsilon$ and $f+\epsilon$.
			\end{itemize}
		\end{rem}
		
		\begin{dfn}[Uniform Convergence (ii)]
			Let $(X,d)$ be a metric space. Let $(f_n)$ be a sequence of real-valued functions defined on a set $E\subseteq X$. The sequence $(f_n)$ \emph{converges uniformly} on $E$ to a function $f$ defined on $E$ if
			\begin{displaymath}
				\lim\limits_{n\rightarrow\infty}\sup\{|f_n(x)-f(x)|: x\in E\}=0.
			\end{displaymath}
		\end{dfn}
		\begin{rem}
			This is an alternative definition to uniform convergence. See details in worksheet 14. We can decide whether a sequence $(f_n)$ converges uniformly to $f$ by calculating $\sup\{|f_n(x)-f(x)|: x\in X\}$ for each $n$. If $f_n-f$ is differentiable, we may use calculus to find these suprema.
		\end{rem}
		
		\begin{eg}
			$f_n(x)=x^n$ does not converge uniformly to $f(x)=\begin{cases}
				0\quad x\in[0,1)\\1\quad x=1.
			\end{cases}$
		\end{eg}
		
		\setcounter{equation}{0}
		\begin{thm}[Uniform Limit Theorem]\label{thm:24.3}
			The uniform limit of a continuous function is continuous. More precisely, let $(f_n)$ be a sequence of real-valued functions
			defined on $E\subseteq X$. Suppose $f_n\rightarrow f$ uniformly on $E$, and suppose $E=\dom(f)$. If $f_n$ is
			continuous at $x_0$ in $E$ for each $n\in\N$, then $f$ is continuous at $x_0$. [so if each $f_n$ is continuous on $S$, then $f$ is continuous on $S$.]
		\end{thm}
		\begin{rem}
			The contrapositive of this theorem is useful to show $f_n$ does not uniformly converge to $f$ on $E$:
			If $f$ is not continuous at $x_0\in E$ but $f_n$ is continuous at $x_0$, then the statement that "$f_n\rightarrow f$ uniformly on $E$" is \textbf{incorrect}.
		\end{rem}
		
		\newpage
		
		\section{More on Uniform Convergence}
		\begin{comment}
		Here are two important facts about integration
		\begin{enumerate}
		\item If $g$ and $h$ are integrable on $[a,b]$ and if $g(x)\leq h(x)$ for all $x\in[a,b]$, then $\int_{a}^{b}g(x)dx\leq\int_{a}^{b}h(x)dx$.
		\item If $g$ is integrable on $[a,b]$, then
		\begin{displaymath}
		\abs{\int_{a}^{b}g(x)dx}\leq\int_{a}^{b}\abs{g(x)}dx.
		\end{displaymath}
		\end{enumerate}
		Also continuous functions on closed intervals are integrable.
		
		\begin{thm}\label{thm:25.2}
		Let $(f_n)$ be a sequence of continuous functions on $[a,b]$, and suppose $f_n\rightarrow f$ uniformly on $[a,b]$. Then
		\begin{displaymath}
		\lim\limits_{n\rightarrow\infty}\int_{a}^{b}f_n(x)dx=\int_{a}^{b}f(x)dx.
		\end{displaymath}
		\begin{proof}
		By \ref{def:24.3} $f$is continuous, so $f_n-f$ are all integrable on $[a,b]$. Let $\epsilon>0$. Since $f_n\rightarrow f$ uniformly on $[a,b]$, there exists $N$ such that $\abs{f_n(x)-f(x)}<\frac{\epsilon}{b-a}$ for all $x\in[a,b]$ and all $n>N$. Consequently $n>N$ implies
		\begin{align*}
		\abs{\int_{a}^{b}f_n(x)dx-\int_{a}^{b}f(x)dx}&=\abs{\int_{a}^{b}[f_n(x)-f(x)]dx}\\
		&\leq\int_{a}^{b}\abs{f_n(x)-f(x)}dx\\
		&\leq\int_{a}^{b}\frac{\epsilon}{b-a}dx\\
		&=\epsilon
		\end{align*}
		Thus $\lim\limits_{n\rightarrow\infty}\int_{a}^{b}f_n(x)dx=\int_{a}^{b}f(x)dx$.
		\end{proof}
		\end{thm}
		\end{comment}
		
		\begin{dfn}[Uniformly Cauchy]
			Let $(X,d)$ be a metric space, and let $E\subseteq X$. A sequence $(f_n)$ of functions defined on a set $E$ is \emph{uniformly Cauchy on S} if
			\begin{align*}
				\text{for each $\epsilon>0$ there exists a number $N$ such that}\\
				\text{$\abs{f_m(x)-f_n(x)}<\epsilon$ for all $x\in E$ and all $m,n\geq N$.}
			\end{align*}
		\end{dfn}
		
		\begin{dfn}[Uniform Convergence for Series of Functions]
			Let $(X,d)$ be a metric space, and let $E\subseteq X$. A series of functions $\sum_{n=1}^{\infty}g_n$ on $E$ is \emph{uniformly convergent to the function $G$ on $E$} if 
			\begin{displaymath}
				\forall\epsilon>0\ \exists N\in\N\ n\geq N\implies\abs{\sum_{k=1}^{n}g_k(x)-G(x)}<\epsilon\text{ for all $x\in E$.}
			\end{displaymath}
		\end{dfn}
		
		\begin{dfn}[Uniform Cauchy Criterion]
			Let $(X,d)$ be a metric space, and let $E\subseteq X$. A series of functions $\sum_{n=1}^{\infty}g_n$ on $E$ satisfy the \emph{uniform Cauchy criterion} if
			\begin{displaymath}
				\forall\epsilon>0\ \exists N\in\N\ n\geq m\geq N\implies \abs{\sum_{k=m}^{n}g_k(x)}<\epsilon\text{ for all $x\in E$.}
			\end{displaymath} 
		\end{dfn}
		\begin{rem}
			There is an analogue between the Cauchy criterion for a normal series $\sum a_k$ and the one for a series of functions $\sum g_k$: The sequence of partial sums of a series $\sum_{k=0}^{\infty}g_k$ of functions is uniformly Cauchy on a set $E$ $\iff$ the series satisfies the \emph{uniform Cauchy criterion} on $E$.
		\end{rem}
		
		\setcounter{equation}{0}
		\begin{thm}\label{thm:25.4}
			Let $(X,d)$ be a metric space, and let $E\subseteq X$. A sequence of functions $(f_n)$ is \emph{uniformly Cauchy} if and only if $(f_n)$ converges uniformly.
		\end{thm}
	
		\begin{cor}
			$\sum_{k=0}^{\infty}g_k$ satisfies the uniform Cauchy criterion if and only if it converges (uniformly).
		\end{cor}
		
		\begin{thm}\label{thm:25.5}
			Consider a series $\sum_{k=0}^{\infty}g_k$ of functions on a set $S\subseteq\R$. Suppose each $g_k$ is continuous on $S$ and the series converges uniformly on $S$. Then the series $\sum_{k=0}^{\infty}g_k$ represents a continuous function on $S$.
		\end{thm}
		
		\begin{thm}\label{thm:25.6}
			If a series $\sum_{k=0}^{\infty}g_k$ of functions satisfies the Cauchy criterion uniformly on a set $S$, then the
			series converges uniformly on $S$.
		\end{thm}
		
		\begin{thm}[Weierstrass M-test]\label{thm:25.7}
			Let $(M_k)$ be a sequence of nonnegative real numbers where $\sum M_k<\infty$, i.e., $\sum M_k$ converges. If $|g_k(x)|\leq M_k$ for all $x$ in a set $E$ and $k\in\N$, then $\sum g_k$ converges uniformly on $E$.
		\end{thm}
		
		\begin{thm}
			If the series $\sum g_n$ converges uniformly on a set $S$, then
			\begin{displaymath}
				\lim\limits_{n\rightarrow\infty}\sup\{|g_n(x)|: x\in S\}=0.
			\end{displaymath}
		\end{thm}
		
		\begin{thm}
			Let $(f_n)$ be a sequnece of bounded functions on a set $S$. If $f_n\rightarrow f$ uniformly on $E$, then $f$ is a bounded function on $E$.
		\end{thm}
		
		\begin{thm}\label{thm:26.1}
			Let $\sum_{n=0}^{\infty}a_n(x-x_0)^n$ be a power series with radius of convergence $R>0$ [possibly $R=+\infty$]. If $0<R_0<R$, then the power series converges uniformly on $[x_0-R_0, x_0+R_0]$ to a continuous function.
		\end{thm}

		\begin{cor}\label{cor:26.2}
			The power series $\sum a_n(x-x_0)^n$ with radius of convergence $R>0$ converges to a continuous function on the open interval $(x_0-R,x_0+R)$.
		\end{cor}
		
		\begin{thm}[Dini's Theorem]
			If $(f_n)$ is a sequence of continous functions on $[a,b]$ such that $(f_n(x))$ is nondecreasing for each $x\in[a,b]$ and $f_n\rightarrow f$ pointwise for some continuous function $f$, then $f_n\rightarrow f$ uniformly on $[a,b]$.
		\end{thm}
		
		\begin{lem}
			If $f(x)=\sum_{n=0}^{\infty}a_nx^n$ has radius of convergence $1$ and the series converges at $x=1$, then $f$ is continuous on $[0,1]$.
		\end{lem}
		
		\setcounter{equation}{0}
		\begin{thm}[Abel's Theorem]\label{thm:26.6}
			Let $f(x)=\sum_{n=0}^{\infty}a_nx^n$ be a power series with finite positive radius of convergence $R$. If the series converges at $x=R$, then $f$ is continuous at $x=R$; if the series converges at $x=-R$, then $f$ is continuous at $x=-R$.
		\end{thm}
		
		\newpage
		
		\begin{comment}
		\section{Differentiation and Integration of Power Series}
		\begin{lem}\label{lem:26.3}
		If the power series $\sum_{n=0}^{\infty}a_nx^n$ has radius of convergence $R$, then the power series
		\begin{displaymath}
		\sum_{n=1}^{\infty}na_nx^{n-1}\quad\text{and}\quad\sum_{n=0}^{\infty}\frac{a_n}{n+1}x^{n+1}
		\end{displaymath} 
		also have radius of convergence $R$.
		\end{lem}
		\begin{proof}
		First observe the series $\sum na_nx^{n-1}$ and $\sum na_nx^n$ have the same radius of convergence. Same for $\sum\frac{a_n}{n+1}x^{n+1}$ and $\sum\frac{a_n}{n+1}x^n$. It is because one is $x$ multiple of the other.
		
		For the series $\sum na_nx^n$, because $\lim\sup(n|a_n|)^{1/n}=\lim\sup n^{1/n}|a_n|^{1/n}$ and $\lim n^{1/n}=1$, so $\lim\sup(n|a_n|)^{1/n}=\lim\sup|a_n|^{1/n}=\beta$. Hence $\sum na_nx^n$ has radius of convergence $R$.
		
		Similarly, for the series $\sum\frac{a_n}{n+1}x^n$, because $\lim\sup(\frac{|a_n|}{n+1})^{1/n}=\lim\sup(\frac{1}{n+1})^{1/n}\cdots|a_n|^{1/n}=1\cdot\lim\sup|a_n|^{1/n}=\beta$ by \ref{thm:12.1}, so the series $\sum\frac{a_n}{n+1}x^n$ has radius of convergence $R$.
		\end{proof}
		
		\begin{thm}\label{thm:26.4}
		Suppose $f(x)=\sum_{n=0}^{\infty}a_nx^n$ has radius of convergence $R>0$. Then
		\begin{displaymath}
		\int_{0}^{x}f(t)dx=\sum_{n=0}^{\infty}\frac{a_n}{n+1}x^{n+1}\quad\text{for}\quad |x|<R
		\end{displaymath}
		\end{thm}
		\begin{proof}
		Fix $x<0$. The case $x>0$ is similar. On the interval $[x,0]$, the sequence of partial sums $\sum_{k=0}^{n}a_kt^k$ converges uniformly to $f(t)$ by \ref{thm:26.1}. Consequently, by \ref{thm:25.2} we have
		\begin{align*}
		\int_{x}^{0}f(t)dt&=\lim_{n\rightarrow \infty}\int_{x}^{0}\left(\sum_{k=0}^{n}a_kt^k\right)dt\\
		&=\lim_{n\rightarrow \infty}\sum_{k=0}^{n}a_k\int_{x}^{0}t^kdt\\
		&=\lim_{n\rightarrow \infty}\sum_{k=0}^{n}a_k\left[\frac{0^{k+1}-x^{k+1}}{k+1}\right]\\
		&=-\left(\sum_{k=0}^{\infty}\frac{a_k}{k+1}x^{k+1}\right)\\
		&=-\left(\int_{0}^{x}f(t)dt\right)
		\end{align*}
		\end{proof}
		The last theorem shows that a power series can be integrated term-by-term inside its interval of convergence. The next theorem shows that term-by-term differentiation is also legal
		
		\begin{thm}\label{thm:26.5}
		Let $f(x)=\sum_{n=0}^{\infty}a_nx^n$ have radius of convergence $R>0$. Then $f$ is differentiable on $(-R,R)$ and \begin{displaymath}
		f^\prime(x)=\sum_{n=1}^{\infty}na_nx^{n-1}\quad\text{for}\quad|x|<R
		\end{displaymath}
		\end{thm}
		\begin{proof}
		We begin with the series $g(x)=\sum_{n=1}^{\infty}na_nx^{n-1}$ and observe this series converges for $|x|<R$ by \ref{lem:26.3}. By \ref{thm:26.4}, we can integrate $g(x)$ term-by-term:
		\begin{displaymath}
		\int_{0}^{x}g(t)dt=\sum_{n=1}^{\infty}a_nx^n=f(x)-a_0\quad\text{for}\quad|x|<R.
		\end{displaymath}
		Thus if $0<R_1<R$, then
		\begin{displaymath}
		f(x)=\int_{-R_1}^{x}g(t)dt+k\quad\text{for}\quad|x|\leq R_1
		\end{displaymath}
		where $k=a_0-\int_{-R_1}^{0}g(t)dt$. Since $g$ is continuous, the Fundamental Theorem of Calculus shows $f$ is differentiable and $f^\prime(x)=g(x)$. Thus
		\begin{displaymath}
		f^\prime(x)=g(x)=\sum_{n=1}^{\infty}na_nx^{n-1}\quad\text{for}\quad|x|\leq R_1
		\end{displaymath}
		\end{proof}
		\end{comment}
	\chapter{Differentiation}
		\newpage
		\section{Basic Properties of the Derivative}
		\subsection*{Limits of functions}
		\begin{dfn}[Limit of function (i)]
			We denote 
			\begin{displaymath}
				\lim_{x\rightarrow c}f(x)=L
			\end{displaymath}
			as that for every sequence $(x_n)\subseteq\dom(f)\backslash\{c\}$ such that $x_n\rightarrow c$ where $c\in\R\cup\{\pm\infty\}$, we have $f(x_n)\rightarrow L$.
		\end{dfn}
		\begin{rem}
			Observe that followed by the definition above, $f$ is continuous at $c$ if and only if $\lim_{x\rightarrow c}f(x)=f(c)$.
		\end{rem}
		
		\begin{dfn}[Limit of function (ii)]
			Alternatively, we can also claim the $\epsilon$-$\delta$ definition of the limit of function as $\lim_{x\rightarrow c}f(x)=L$ if
			\begin{displaymath}
				\forall\epsilon>0\ \exists\delta>0\ 0<\abs{x-c}<\delta\implies\abs{f(x)-L}<\epsilon
			\end{displaymath}	
		\end{dfn}
		
		\begin{dfn}[Left-hand Limit]
			We write $\lim_{x\rightarrow c^-}f(x)=L$ if there exists $a<c$ such that $(a,c)\subseteq\dom(f)$ and for any sequence $(x_n)\subseteq(a,c)$ such that $x_n\rightarrow c$, we have $f(x_n)\rightarrow L$.
		\end{dfn}
		
		\begin{thm}
			$\lim_{x\rightarrow c}f(x)$ exists if and only if $\lim_{x\rightarrow c^-}f(x)=\lim_{x\rightarrow c^+}f(x)$, in which case, all equal.
		\end{thm}
		\newpage
		\lline
		
		\subsection*{Derivative}
		\begin{dfn}\label{def:28.1}
			Let $f$ be a real-valued function defined on an open interval containing a point $x$. Define \emph{difference quotient} on $\dom(f)\backslash\{x\}$ as 
			\begin{displaymath}
				\varphi_x(y)=\frac{f(y)-f(x)}{y-x}.
			\end{displaymath}
			We say $f$ is \emph{differentiable} at $x$, or $f$ has a derivative at $a$, if the limit
			\begin{displaymath}
				\lim_{y\rightarrow x}\varphi_x(y)=\lim_{y\rightarrow x}\frac{f(y)-f(x)}{y-x}
			\end{displaymath}
			exists and is finite. We will write $f^\prime(x)$ for the derivative of $f$ at $x$:
			\begin{displaymath}
				f^\prime(x)=\lim_{y\rightarrow x}\varphi_x(y)=\lim_{y\rightarrow x}\frac{f(y)-f(x)}{y-x}
			\end{displaymath}
			whenever this limit exists and is finite.
		\end{dfn}
		\begin{rem}
			\begin{itemize}
				\item $f$ is \emph{differentiable on a set $E$} if $f$ is differentiable at every point $x\in E$.
				\item $f$ is \emph{differentiable} if $f$ is differentiable at every point $x\in\dom(f)$.
				\item We can consider $f^\prime$ as a function. The domain of $f^\prime$ is the set of points at which $f$ is differentiable; thus $\dom(f^\prime)\subseteq\dom(f)$.
			\end{itemize}
		\end{rem}
		
		\begin{thm}\label{thm:28.2}
			If $f$ is differentiable at a point $x$, then $f$ is continuous at $x$.
		\end{thm}
		\newpage
		
		\begin{thm}\label{def:28.3}
			Let $f$ and $g$ be functions that are differentiable at the point $a$. Each of the functions $cf$, $f+g$, $fg$, and $f/g$ is also differentiable at $a$, except $f/g$ is $g(a)=0$ since $f/g$ is not defined at $a$ in this case.
			The formulas are:
			\begin{enumerate}[(i)]
				\item $(cf)^\prime(a)=c\cdot f^\prime(a)$;
				\item $(f+g)^\prime(a)=f\prime(a)+g^\prime(a)$;
				\item Product rule: $(fg)^\prime(a)=f(a)g^\prime(a)+f^\prime(a)g(a)$;
				\item Quotient rule: $(f/g)^\prime(a)=\frac{[g(a)f^\prime(a)-f(a)g^\prime(a)]}{g^2(a)}$ if $g(a)\neq0$.
			\end{enumerate}
		\end{thm}
		
		\begin{thm}[Chain Rule]\label{def:28.4}
			If $f$ is differentiable at $a$ and $g$ is differentiable at $f(a)$, then the composite function $g\circ f$ is differentiable at $a$ and we have $(g\circ f)^\prime(a)=g^\prime(f(a))\cdot f^\prime(a)$.
		\end{thm}
		If $f$ is differentiable on an interval $I$ and if $g$ is differentiable on $\{f(x): x\in I\}$, then $(g\circ f)^\prime$ is exactly $(g^\prime\circ f)\cdot f^\prime$ on $I$.
		\newpage
		
		\section{The Mean Value Theorem}
		\begin{lem}
		Let $f$ be defined on an open interval $(a,b)$ containing $x$. If $f$ attains its maximum (or minimum) at $x\in(a,b)$ and $f$ is differentiable at $x$, then $f^\prime(x)=0$.
		\end{lem}
		\begin{itemize}
			\item If $f^\prime(x)\neq0$, then $f$ does not attains its maximum nor minimum at $x$.
			\item The theorem does not apply to endpoints of (half) closed intervals.
		\end{itemize}
		
		\begin{thm}[Rolle's Theorem]\label{thm:29.2, Rolle}
			Suppose $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$, and that $f(a)=f(b)$. Then there exists $x\in(a,b)$ such that $f^\prime(x)=0$.
		\end{thm}
		
		\begin{thm}[Mean Value Theorem]\label{thm:29.3, MVT}
			Let $f$ be a continuous function on $[a,b]$ which is differentiable on $(a,b)$. Then there exists $x\in(a,b)$ such that $f^\prime(x)=\frac{f(b)-f(a)}{b-a}$.
		\end{thm}

		\begin{cor}
			If $f$ is differentiable on $(a,b)$, and $f^\prime(x)=0$ for all $x\in(a,b)$, then $f$ is constant on $(a,b)$.
		\end{cor}

		\begin{cor}
			If $f,g$ are differentiable on $(a,b)$ and $f^\prime(x)=g^\prime(x)$ for all $x\in(a,b)$, then $f=g+C$ for some $c\in\R$.
		\end{cor}
		\newpage
		\begin{cor}
			Let $f$ be a differentiable function on $(a,b)$.
			\begin{enumerate}[(i)]
				\item If $f^\prime(x)>0$ for all $x\in(a,b)$, then $f$ is strictly increasing;
				\item If $f^\prime(x)<0$ for all $x\in(a,b)$, then $f$ is strictly decreasing; 
			\end{enumerate}
		\end{cor}
		\begin{rem}
			The converse is false generally. Consider $f(x)=x^3$ at $0$.
		\end{rem}
		
		\begin{thm}[Generalized Mean Value Theorem]
			Suppose $f$ and $g$ are continuous on $[a,b]$ and differentiable on $(a,b)$. Prove that there exists $x\in(a,b)$ such that
			\begin{displaymath}
				f^\prime(x)(g(b)-g(a))=g^\prime(x)(f(b)-f(a)).
			\end{displaymath}
		\end{thm}
		
		\begin{thm}
			If $f$ is a differentiable function on $(a,b)$ with bounded derivative, then $f$ is uniformly continuous on $(a,b)$.
		\end{thm}
		\begin{rem}
			The converse does not hold generally. For example, consider $f(x)=\sqrt{x}$ on $(0,1)$. 
		\end{rem}
		
		\begin{thm}[Intermediate Value Theorem for Derivatives]
			Let $f$ be a differentiable function on $(a,b)$. If $a<x<y<b$ and $c$ is between $f^\prime(x)$ and $f^\prime(y)$, then there exists $z\in(x,y)$ such that $f^\prime(z)=c$. 
		\end{thm}
		\newpage
		
		\section{L'Hospital's Rule}
		\begin{thm}
			Suppose $f,g$ are differentiable on $(a,b)$ and $g^\prime(x)\neq0$ for where $-\infty\leq a<b\leq\infty$. Let $s\in\{a,b\}$. If $\lim_{x\rightarrow s}\frac{f^\prime(x)}{g^\prime(x)}=L$ ($-\infty\leq L\leq \infty$) and either
			\begin{enumerate}[(i)]
				\item $\lim_{x\rightarrow s}f(x)=\lim_{x\rightarrow s}g(x)=0$; or
				\item $\lim_{x\rightarrow s}g(x)=\pm\infty$,
			\end{enumerate} 
			then $\lim_{x\rightarrow s}\frac{f(x)}{g(x)}=L$.
		\end{thm}
		\newpage
		\section{Taylor's Theorem}
		\subsection*{Taylor Series}
		\begin{nte}
			$f^{(n)}$ denotes the $n^\text{th}$ derivative of $f$.
		\end{nte}
		
		\begin{dfn}[Infinitely Differentiable]
			$f$ is \emph{infinitely differentiable} at $x_0$ if $f^{(n)}(x_0)$ exists for all $n\in\N$.
		\end{dfn}
		\begin{rem}
			The existence of $f^{(n)}(x_0)$ implies $f^{(n-1)}$ exists on an open interval containing $x_0$.
		\end{rem}
		
		\begin{dfn}[Taylor Series]
			Let $f$ be a function defined on an open interval $I$ containing $x_0$. If $f$ is infinitely differentiable at $x_0$, define the \emph{Taylor series for $f$ about $x_0$} as the power series
			\begin{displaymath}
				T^{f,x_0}(x):=\sum_{k=0}^{\infty}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k
			\end{displaymath}
			on the interval of convergence of the Taylor series.
		\end{dfn}
		
		\begin{dfn}[$n^\text{th}$ Remainder]
			The $n^\text{th}$ \emph{remainder} (of the above) is 
			\begin{displaymath}
				R_n^{f,x_0}(x)=f(x)-\sum_{k=0}^{n-1}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k.
			\end{displaymath}
		\end{dfn}
		\begin{rem}
			Observe that for any $x\in I$, $f(x)=\sum_{k=0}^{\infty}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k\iff R_n^{f,x_0}(x)\rightarrow0$ as $n\rightarrow\infty$.
		\end{rem}
		\lline
		\newpage
		\subsection*{Taylor's theorem}
		\begin{thm}[Taylor's Theorem]
			Let $f$ be defined on an open interval $I$ containing $x_0$ such that the $n^\text{th}$ derivative of $f$ exists (i.e. first $n$ derivatives of $f$ exist) at every point in $I$. Then for each $x\in I\backslash\{x_0\}$, there exists $\alpha_x$ between $x$ and $x_0$ such that
			\begin{displaymath}
				R_n^{f,x_0}(x)=\frac{f^{(n)}(\alpha_x)}{n!}(x-x_0)^n,
			\end{displaymath}
			i.e.
			\begin{displaymath}
				f(x)=\sum_{k=0}^{n-1}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k+\frac{f^{(n)}(\alpha_x)}{n!}(x-x_0)^n
			\end{displaymath}
		\end{thm}
		
		\begin{cor}
			If $f$ is infinitely differentiable on an interval $I$ containing $x_0$ and there exists $M>0$ such that $\abs{f^{(n)}(x)}\leq M$ for all $n\geq0, x\in I$. Then $f(x)=T^{f,x_0}(x)$, i.e. $R_n^{f,x_0}(x)\rightarrow0$, for all $x\in I$.
		\end{cor}
	
	\chapter{Integration}
	\newpage
	\section{The Riemann Integral}
	\begin{nte}
		Let $f$ be a bounded function on a closed interval $[a,b]$. For $S\subseteq[a,b]$, define
		\begin{displaymath}
			M(f,S)=\sup\{f(x):x\in S\}\quad\text{and}\quad m(f,S)=\inf\{f(x):x\in S\}.
		\end{displaymath}
		i.e. $M(f,S)$ is the least upper bound of $f$ on $S$, and $m(f,S)$ is the greatest lower bound of $f$ on $S$.	
	\end{nte}
	
	\begin{dfn}[Partition]
		A \emph{partition} of $[a,b]$ is a finite ordered subset of the form
		\begin{displaymath}
			P=\{a=t_0<t_1<t_2<\cdots<t_n=b\}.
		\end{displaymath}
		e.g. $P=\{0,\frac{1}{4},\frac{1}{2},1\}$ is a partition of $[0,1]$.
	\end{dfn}
	
	\begin{dfn}[Darboux Sum]
		The \emph{upper Darboux sum} $U(f,P)$ of $f$ w.r.t. $P=\{t_0,\dots,t_n\}$ is
		\begin{displaymath}
			U(f,P)=\sum_{k=1}^{n}M(f,I_k)\cdot\ell(I_k)\quad\text{where $I_k=[t_{k-1},t_k]$,\ $\ell(I_k)=t_k-t_{k-1}$}.
		\end{displaymath}
		The \emph{lower Darboux sum} $L(f,P)$ of $f$ w.r.t. $P=\{t_0,\dots,t_n\}$ is
		\begin{displaymath}
			L(f,P)=\sum_{k=1}^{n}m(f,I_k)\cdot\ell(I_k)\quad\text{where $I_k=[t_{k-1},t_k]$,\ $\ell(I_k)=t_k-t_{k-1}$}.
		\end{displaymath}
	\end{dfn}
	\begin{rem}
		Observe that
		\begin{displaymath}
			-\infty<m(f,[a,b])(b-a)\leq L(f,P)\leq U(f,P)\leq M(f,[a,b])(b-a)<\infty
		\end{displaymath}
	\end{rem}
	
	\begin{dfn}[Darboux Integral]
		The \emph{upper Darboux integral} $U(f)$ of $f$ over $[a,b]$ is
		\begin{displaymath}
			U(f)=\inf\{U(f,P):P\in\Pi_{[a,b]}\}\quad\text{where $\Pi_{[a,b]}$ is the set of all partitions of $[a,b]$}.
		\end{displaymath}
		The \emph{lower Darboux integral} $L(f)$ of $f$ over $[a,b]$ is
		\begin{displaymath}
			L(f)=\sup\{L(f,P):P\in\Pi_{[a,b]}\}\quad\text{where $\Pi_{[a,b]}$ is the set of all partitions of $[a,b]$}.
		\end{displaymath}
	\end{dfn}
	\begin{rem}
		By the previous observation, $U(f),L(f)\in\R$. 
	\end{rem}
	
	\begin{dfn}[Refinement]
		If partitions $P,P^\ast\in\Pi_{[a,b]}$ and $P\subseteq P^\ast$, $P^\ast$ is called a \emph{refinement} of $P$.
	\end{dfn}
	
	\begin{lem}
		Let $f$ be a bounded function. If $P^\ast$ is a refinement of $P$, then
		\begin{displaymath}
			L(f,P)\leq L(f,P^\ast)\leq U(f,P^\ast)\leq U(f,P).
		\end{displaymath}
	\end{lem}
	
	\begin{lem}
		Let $f$ be a bounded function. If $P,Q\in\Pi_{[a,b]}$, then $L(f,P)\leq U(f,Q)$. 
	\end{lem}
	
	\begin{thm}
		$L(f)\leq U(f)$. 
	\end{thm}
	
	\begin{dfn}[Integrable]
		$f$ is \emph{integrable/Darboux Integrable/Riemann Integrable} if $L(f)=U(f)$. 
	\end{dfn}
	
	\begin{lem}
		Let $f$ and $g$ be two bounded functions on $[a,b]$. Then
		\begin{enumerate}[(i)]
			\item $\inf\{U(f,P)+U(g,P): P\in\Pi_{[a,b]}\} = \inf\{U(f,P): P\in\Pi_{[a,b]}\}+\inf\{U(g,P): P\in\Pi_{[a,b]}\};$   
			\item $\sup\{L(f,P)+L(g,P): P\in\Pi_{[a,b]}\} = \sup\{U(f,P): P\in\Pi_{[a,b]}\}+\sup\{L(g,P): P\in\Pi_{[a,b]}\}$.
		\end{enumerate}
	\end{lem}
	
	\begin{nte}
		We'll use better notations for $U_a^b(f)=U(f)$ meaning the upper Darboux integral of $f$ over $[a,b]$. Similarly for $L_a^b(f)=L(f)$. 
	\end{nte}
	
	\begin{dfn}[Integral]
		If $f$ is \emph{integrable} on $[a,b]$, we define the (Riemann or Darboux) \emph{integral of f} on $[a,b]$ as 
	\end{dfn}
	\begin{eg}
		On $[0,1]$, $f(x)=\begin{cases}1\quad\text{$x\in\Q$}\\0\quad\text{$x\notin\Q$}\end{cases}$. For any partition $P\in\Pi_{[0,1]}$, $L(f,P)=0\implies L(f)=0$ and $U(f,P)=1\implies U(f)=1$, so $f$ is not Riemann/Darboux integrable. 
	\end{eg}
	
	\begin{dfn}[Mesh]
		The \emph{mesh} of a partition $P=\{a=t_0<t_1<\cdots<t_{n-1}t_n=b\}$ is
		\begin{displaymath}
			\text{mesh}(P)=\max\{t_1-t_0,t_2-t_1,\dots,t_n-t_{n-1}\}=\text{the length of the longest subinterval}
		\end{displaymath}
	\end{dfn}
	
	\begin{thm}
		A bounded function $f$ on $[a,b]$ is integrable if and only if for each $\epsilon>0$ there exists a partition $P$ of $[a,b]$ such that
		\begin{displaymath}
			U(f,P)-L(f,P)<\epsilon.
		\end{displaymath}
	\end{thm}
	
	\begin{thm}
		A bounded function $f$ on $[a,b]$ is integrable if and only if for each $\epsilon>0$ there exists a $\delta>0$ such that
		\begin{displaymath}
			\mesh(P)<\delta\implies U(f,P)-L(f,P)<\epsilon
		\end{displaymath}
		for all partitions $P$ of $[a,b]$.
	\end{thm}
	\newpage
	
	\section{Properties of the Riemann Integral}
	\begin{thm}
		Every monotonic function $f$ on $[a,b]$ is integrable.
	\end{thm}

	\begin{thm}
		Every continuous function $f$ on $[a,b]$ is integrable.
	\end{thm}
	
	\begin{thm}[Scalar Multiple and Sum]
		Let $f,g$ be integrable functions on $[a,b]$ and let $c\in\R$. Then
		\begin{enumerate}[(i)]
			\item $cf$ is integrable and $\int_{a}^{b}(cf)(x)dx=c\int_{a}^{b}f(x)dx.$
			\item $f+g$ is integrable and $\int_{a}^{b}(f+g)(x)dx=\int_{a}^{b}f(x)dx+\int_{a}^{b}g(x)dx.$
		\end{enumerate}
	\end{thm}
	
	\begin{thm}
		If $f$ and $g$ are integrable on $[a,b]$, then $\max(f,g)$ is integrable on $[a,b]$.
	\end{thm}
	
	\begin{thm}
		\begin{enumerate}[(i)]
			\item If $f$ and $g$ are integrable on $[a,b]$ and $f(x)\leq g(x)$ for all $x\in[a,b]$, then $\int_{a}^{b}f(x)dx\leq\int_{a}^{b}g(x)dx$.
			\item If $g$ is continuous on $[a,b]$ and $g(x)\geq 0$ for all $x\in[a,b]$ and $\int_{a}^{b}g(x)dx=0$, then $g$ is the zero function on $[a,b]$.
		\end{enumerate}
	\end{thm}
	\begin{rem}
		An useful contrapositive of (ii) is that ``If $f$ is not a zero function on $[a,b]$ (i.e. not identically zero) but $f$ is continuous and nonnegative, then $\int_{a}^{b}f(x)dx>0$.  
	\end{rem}
	
	\begin{thm}
		If $f$ is integrable on $[a,b]$, then $|f|$ is integrable on $[a,b]$ and
		\begin{displaymath}
			\abs{\int_{a}^{b}f}\leq\int_{a}^{b}\abs{f}.
		\end{displaymath}
	\end{thm}
	
	\begin{thm}
		If $f$ is integrable on $[a,b]$ and $f$ is integrable on $[b,c]$, then $f$ is integrable on $[a,c]$ and 
		\begin{displaymath}
			\int_{a}^{c}f(x)dx=\int_{a}^{b}f(x)dx+\int_{b}^{c}f(x)dx.
		\end{displaymath}
	\end{thm}
	
	\begin{thm}[Intermediate Value Theorem for Integrals]
		If $f$ is continuous on $[a,b]$, then there exists $x_0\in(a,b)$ such that
		\begin{displaymath}
			f(x_0)=\frac{1}{b-a}\int_{a}^{b}f(x)dx.
		\end{displaymath}
	\end{thm}

	\newpage
	\section{Fundamental Theorem of Calculus}
	\begin{dfn}
		A bounded function $f$ on $(a,b)$ is \emph{integrable on $[a.b]$} if any extension of $f$ to $[a,b]$ is integrable.
	\end{dfn}
	
	\begin{thm}[Fundamental Theorem of Calculus I]
		If $f$ is integrable on $[a,b]$ and $F$ is continuous on $[a,b]$ and differentiable on $(a,b)$ and $F^\prime(x)=f(x)$ (i.e. $F$ is an antiderivative of $f$ on $(a,b)$) for all $x\in(a,b)$, then
		\begin{displaymath}
			\int_{a}^{b}f(x)=F(b)-F(a)
		\end{displaymath}
	\end{thm}
	
	\begin{thm}[Fundamental Theorem of Calculus II]
		Let $f$ be integrable on $[a,b]$. For $x$ in $[a,b]$, let $F(x)=\int_{a}^{x}f(t)dt$. Then
		\begin{enumerate}[(i)]
			\item $F$ is continuous on $[a,b]$.
			\item If $f$ is continuous at $x_0\in(a,b)$, then $F$ is differentiable at $x_0$ and $F^\prime(x_0)=f(x_0)$. i.e. $f$ is continuous on $[a,b]\implies F$ is an antiderivative of $f$ on $(a,b)$. 
		\end{enumerate}
	\end{thm}

	\chapter{Useful Tricks}
	\begin{enumerate}
		\item Here is one of the most important techniques in real analysis.
		\begin{enumerate}[(a)]
			\item If $a\leq b+\epsilon$ for any $\epsilon>0$, then $a\leq b$.
			\item If $a\geq b-\epsilon$ for any $\epsilon>0$, then $a\geq b$.
			\item If $|a-b|<\epsilon$ for any $\epsilon>0$, then $|a-b|=0$.
		\end{enumerate}
		\item Let $S$ be a bounded nonempty subset of $\R$ and suppose $\sup S\notin S$. Then there is a (strictly) increasing sequence $(s_n)$ of points in $S$ such that $\lim s_n=\sup S$.
		\item A point $x$ is a limit point of a set $E\subseteq X$ if and only if $x=\lim x_n$ for some sequence $x_n$ of points in $E\backslash\{x\}$. 
		\item Let $(s_n)$ be a convergent sequence.
		\begin{itemize}
			\item If $s_n\geq a$ for all but finitely many $n$, then $\lim s_n\geq a$.
			\item If $s_n\leq b$ for all but finitely many $n$, then $\lim s_n\leq b$.
		\end{itemize}
		\item (Squeeze Theorem) If $a_n\leq s_n\leq b_n$ for all $n$ and $\lim a_n=\lim b_n=s$, then $\lim s_n=s$.
		\item Assume all $s_n\neq0$ and that the limit $L=\lim\left|\frac{s_{n+1}}{s_n}\right|$ exists.
		\begin{enumerate}[(a)]
			\item If $L<1$, then $\lim s_n=0$.
			\item If $L>1$, then $\lim |s_n|=+\infty$.
		\end{enumerate} 
		\item The set $\mathbb{Q}$ of rational number can be listed as a sequence $(r_n)$. Given any real number $a$ there exists a subsequence $(r_{n_k})$ of $(r_n)$ converging to $a$.
		\item Given two \textbf{convergent} sequences $(s_n)$ and $(t_n)$. If there exists $N\in\N$ such that $s_n\leq t_n$ for all $n\geq N$, then $\lim s_n\leq\lim t_n$.
		\item In general, if $A\subseteq B$, then $\inf A\geq \inf B$ and $\sup A\leq \sup B$.
		\item Classic artificial functions for questions related to Intermediate Value Theorem/Mean Value Theorem:
		\begin{itemize}
			\item $h(x)=(f(b)-f(a))g(x)-(g(b)-g(a))f(x)$.
			\item $g(x)=f(x)-x$ for some $c\in\R$.
			\item $g(x)=f(x+c)-f(x)$ for some $c\in\R$.
		\end{itemize}
		\item Take the function to the power of Euler's number $E$ and use L'Hospital's rule. e.g. $\lim_{x\rightarrow0^+}x^x=\lim_{x\rightarrow0^+}e^{x\log x}$.
		\item To show the integration of an integrable function $f$ on $[a,b]$, $\int_{a}^{b}f$ is equal to some form like $F$, we can show that both $\int_{a}^{b}f$ and $F$ are between $L(f,P)$ and $U(f,P)$ since $U(f,P)-L(f,P)<\epsilon\implies \abs{\int_{a}^{b}f-F}<\epsilon$ for each $\epsilon>0$.
	\end{enumerate}
	
\end{document}